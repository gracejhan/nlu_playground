{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1, DS-GA 1012, Spring 2019\n",
    "\n",
    "## Grace Han (jh5990@nyu.edu)\n",
    "\n",
    "## Due Feburary 13, 2019 at 2pm (ET)\n",
    "\n",
    "Download the data zip `DS-GA1012-hw1-data.zip`. Complete the following questions in the notebook and submit your completed notebook on NYU Classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring effect of context size [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We face many implicit and explicit design decisions in creating distributional word representations. For example, in lecture and in lab, we created word vectors using a co-occurence matrix built on neighboring pairs of words. We might suspect, however, that we can get more signal of word similarity by considering larger contexts than pairs of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a__. Write `build_cooccurrence_matrix`, which generates the co-occurence matrix for a window of arbitrary size and for the vocabulary of `max_vocab_size` most frequent words. Feel free to modify the code used in lab [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def load_sst(data_file):\n",
    "    with open(data_file, 'r') as data_fh:\n",
    "        data_fh.readline() # skip the header\n",
    "        data = [r.split('\\t')[1] for r in data_fh.readlines()]\n",
    "    return data # a list of sentences\n",
    "\n",
    "def build_cooccurrence_matrix(data, max_vocab_size=20000, context_size=1):\n",
    "    \"\"\" Build a co-occurrence matrix\n",
    "    \n",
    "    args:\n",
    "        - data: iterable where each item is a list of tokens (string) \n",
    "        - max_vocab_size: maximum vocabulary size\n",
    "        - context_size: window around a word that is considered context\n",
    "            context_size=1 should consider pairs of adjacent words\n",
    "            \n",
    "    returns:\n",
    "        - co-occurrence matrix: numpy array where row i corresponds to the co-occurrence counts for word i\"\"\"\n",
    "    def get_token_frequencies():\n",
    "        tok2freq = defaultdict(int) # frequency of each token in the entire data\n",
    "        coocur_counts = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        for datum in data:\n",
    "            tokens = datum.strip().split()\n",
    "            for i, tok in enumerate(tokens):\n",
    "                tok2freq[tok] += 1\n",
    "                coocur_counts[tok][tok] += 1\n",
    "                if i < len(tokens) - context_size:\n",
    "                    for j in range(1, context_size+1):\n",
    "                        coocur_counts[tok][tokens[i + j]] += 1\n",
    "                        coocur_counts[tokens[i + j]][tok] += 1\n",
    "                elif i < len(tokens) - 1: # compute co-occurence for last context_size tokens\n",
    "                    for j in range(1, len(tokens)-i):\n",
    "                        coocur_counts[tok][tokens[i + j]] += 1\n",
    "                        coocur_counts[tokens[i + j]][tok] += 1\n",
    "                            \n",
    "        return tok2freq, coocur_counts\n",
    "    \n",
    "    def prune_vocabulary(tok2freq, max_vocab_size):\n",
    "        \"\"\" Prune vocab by taking max_vocab_size most frequent words \"\"\"\n",
    "        tok_and_freqs = [(k, v) for k, v in tok2freq.items()]\n",
    "        tok_and_freqs.sort(key = lambda x: x[1], reverse=True) # sorts in-place\n",
    "        tok2idx = {tok: idx for idx, (tok, _) in enumerate(tok_and_freqs[:max_vocab_size])}\n",
    "        idx2tok = {idx: tok for tok, idx in tok2idx.items()}\n",
    "        return tok2idx, idx2tok\n",
    "    \n",
    "    def _build_coocurrence_mat(idx2tok, coocur_counts):\n",
    "        vocab_size = len(idx2tok)\n",
    "        mat = [[0 for _ in range(vocab_size)] for _ in range(vocab_size)]\n",
    "        for i in range(vocab_size - 1):\n",
    "            for j in range(i, vocab_size):    # start from i instead of i+1 to fill in the diagonal matrix\n",
    "                if coocur_counts[idx2tok[i]][idx2tok[j]]:\n",
    "                    mat[i][j] = coocur_counts[idx2tok[i]][idx2tok[j]]\n",
    "                    mat[j][i] = coocur_counts[idx2tok[i]][idx2tok[j]]\n",
    "        return np.array(mat)        \n",
    "\n",
    "    \n",
    "    print(\"Counting words...\")\n",
    "    start_time = time.time()\n",
    "    tok2freq, coocur_counts = get_token_frequencies()\n",
    "    print(\"\\tFinished counting words in %.5f\" % (time.time() - start_time))\n",
    "\n",
    "    print(\"Pruning vocabulary...\")\n",
    "    tok2idx, idx2tok = prune_vocabulary(tok2freq, max_vocab_size)\n",
    "    start_time = time.time()\n",
    "    print(\"\\tFinished pruning vocabulary in %.5f\" % (time.time() - start_time))\n",
    "    \n",
    "    print(\"Building co-occurrence matrix...\")\n",
    "    start_time = time.time()\n",
    "    coocur_mat = _build_coocurrence_mat(idx2tok, coocur_counts)\n",
    "    print(\"\\tFinished building co-occurrence matrix in %.5f\" % (time.time() - start_time))\n",
    "    return coocur_mat, tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your implementation of `build_cooccurrence_matrix` to generate the co-occurence matrix from the sentences of [SST](http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip) (file `datasetSentences.txt`) with `context_size=2` and `max_vocab_size=10000`. What is the co-occurrence count of the words \"the\" and \"end\"? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words...\n",
      "\tFinished counting words in 0.69655\n",
      "Pruning vocabulary...\n",
      "\tFinished pruning vocabulary in 0.00000\n",
      "Building co-occurrence matrix...\n",
      "\tFinished building co-occurrence matrix in 26.71117\n"
     ]
    }
   ],
   "source": [
    "data_file = 'HW1/datasetSentences.txt'\n",
    "\n",
    "data = load_sst(data_file)\n",
    "mat, tok2idx, idx2tok = build_cooccurrence_matrix(data, max_vocab_size=10000, context_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11164,    95,   636, ...,     0,     0,     1],\n",
       "       [   95, 11179,  1383, ...,     0,     1,     0],\n",
       "       [  636,  1383,  8435, ...,     0,     0,     0],\n",
       "       ..., \n",
       "       [    0,     0,     0, ...,     2,     0,     0],\n",
       "       [    0,     1,     0, ...,     0,     2,     0],\n",
       "       [    1,     0,     0, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the diagonal elements in the matrix\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence count of \"the\" and \"end\" : 98\n"
     ]
    }
   ],
   "source": [
    "print(\"Co-occurrence count of \\\"the\\\" and \\\"end\\\" : %d\" % mat[tok2idx['the']][tok2idx['end']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b__. Plot the effect of varying context size in $\\{1, 2, 3, 4\\}$ (leaving all the other settings the same) on the quality of the learned word embeddings, as measured by performance (Spearman correlation) on the word similarity dataset [MTurk-771](http://www2.mta.ac.il/~gideon/mturk771.html) between human judgments and cosine similarity of the learned word vectors (see lab). [12 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_length(u):\n",
    "    return np.sqrt(np.dot(u, u))\n",
    "\n",
    "def cosine(u, v):        \n",
    "    return 1.0 - (np.dot(u, v) / (vector_length(u) * vector_length(v)))\n",
    "\n",
    "def prob_norm(u):\n",
    "    return u / np.sum(u)\n",
    "\n",
    "def rowwise_norm_mat(mat):\n",
    "    return np.array([prob_norm(u) for u in mat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def load_word_similarity_dataset(data_file):\n",
    "    with open(data_file, 'r') as data_fh:\n",
    "        raw_data = data_fh.readlines()\n",
    "    data = []\n",
    "    trgs = []\n",
    "    for datum in raw_data:\n",
    "        datum = datum.strip().split(',')\n",
    "        data.append((datum[0], datum[1]))\n",
    "        trgs.append(float(datum[2]))\n",
    "    return data, trgs\n",
    "\n",
    "def evaluate_word_similarity(word_pairs, targets, mat, tok2idx):\n",
    "    preds = []\n",
    "    trgs = []\n",
    "    n_exs = 0\n",
    "    for (word1, word2), trg in zip(word_pairs, targets):\n",
    "        if word1 in tok2idx and word2 in tok2idx:\n",
    "            pred_sim = 1 - cosine(mat[tok2idx[word1]], mat[tok2idx[word2]])\n",
    "            preds.append(pred_sim)\n",
    "            trgs.append(trg)\n",
    "            n_exs += 1\n",
    "    \n",
    "    rho, pvalue = spearmanr(trgs, preds)\n",
    "    print(\"Evaluated on %d of %d examples\" % (n_exs, len(word_pairs)))\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'HW1/MTURK-771.csv'\n",
    "test_data, test_trgs = load_word_similarity_dataset(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Context size: 1\n",
      "Counting words...\n",
      "\tFinished counting words in 0.41459\n",
      "Pruning vocabulary...\n",
      "\tFinished pruning vocabulary in 0.00000\n",
      "Building co-occurrence matrix...\n",
      "\tFinished building co-occurrence matrix in 52.23208\n",
      "* Context size: 2\n",
      "Counting words...\n",
      "\tFinished counting words in 1.59681\n",
      "Pruning vocabulary...\n",
      "\tFinished pruning vocabulary in 0.00000\n",
      "Building co-occurrence matrix...\n",
      "\tFinished building co-occurrence matrix in 29.70679\n",
      "* Context size: 3\n",
      "Counting words...\n",
      "\tFinished counting words in 0.99853\n",
      "Pruning vocabulary...\n",
      "\tFinished pruning vocabulary in 0.00000\n",
      "Building co-occurrence matrix...\n",
      "\tFinished building co-occurrence matrix in 30.64419\n",
      "* Context size: 4\n",
      "Counting words...\n",
      "\tFinished counting words in 2.65033\n",
      "Pruning vocabulary...\n",
      "\tFinished pruning vocabulary in 0.00000\n",
      "Building co-occurrence matrix...\n",
      "\tFinished building co-occurrence matrix in 35.12511\n"
     ]
    }
   ],
   "source": [
    "print(\"* Context size: 1\")\n",
    "mat1, tok2idx1, idx2tok1 = build_cooccurrence_matrix(data, max_vocab_size=10000, context_size=1)\n",
    "print(\"* Context size: 2\")\n",
    "mat2, tok2idx2, idx2tok2 = build_cooccurrence_matrix(data, max_vocab_size=10000, context_size=2)\n",
    "print(\"* Context size: 3\")\n",
    "mat3, tok2idx3, idx2tok3 = build_cooccurrence_matrix(data, max_vocab_size=10000, context_size=3)\n",
    "print(\"* Context size: 4\")\n",
    "mat4, tok2idx4, idx2tok4 = build_cooccurrence_matrix(data, max_vocab_size=10000, context_size=4)\n",
    "\n",
    "norm_mat1 = rowwise_norm_mat(mat1)\n",
    "norm_mat2 = rowwise_norm_mat(mat2)\n",
    "norm_mat3 = rowwise_norm_mat(mat3)\n",
    "norm_mat4 = rowwise_norm_mat(mat4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 248 of 771 examples\n",
      "Evaluated on 248 of 771 examples\n",
      "Evaluated on 248 of 771 examples\n",
      "Evaluated on 248 of 771 examples\n"
     ]
    }
   ],
   "source": [
    "rho1 = evaluate_word_similarity(test_data, test_trgs, norm_mat1, tok2idx)\n",
    "rho2 = evaluate_word_similarity(test_data, test_trgs, norm_mat2, tok2idx)\n",
    "rho3 = evaluate_word_similarity(test_data, test_trgs, norm_mat3, tok2idx)\n",
    "rho4 = evaluate_word_similarity(test_data, test_trgs, norm_mat4, tok2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115270b38>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_sizes = [1,2,3,4]\n",
    "similarities = [rho1, rho2, rho3, rho4]\n",
    "\n",
    "plt.scatter(context_sizes, similarities)\n",
    "plt.xlabel('Context Size')\n",
    "plt.ylabel('Similarity')\n",
    "for x, y in zip(context_sizes, similarities):\n",
    "    plt.text(x, y, str(round(y, 4)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c__. Briefly discuss the pros and cons of varying (i) the context size (ii) the vocabulary size (iii) using bigrams instead of unigrams (iv) using subword tokens instead of words. [8 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) Varying the context size: A small context size lets you discover the syntactic characteristic of the target word. For example which words often appear side by side with a certain word. But this may not capture the semantic characteristic of the word. For example, in what type of sentences the word frequently appears in. For big context size the pros and cons will be the opposite.\n",
    "\n",
    "(ii) Varying the vocabulary size: A small vocaublary size will let you focus on the high frequency words in smaller dimension. It will be good if you are only looking into those words. However, if you want to consider a bigger range of words you should consider a bigger vocabulary but you will have to work with a high dimensional and sparse matrix.\n",
    "\n",
    "(iii) Using bigrams instead of unigrams: By using bigrams ordering of words will be taken into account. However, the maximum vocabulary size becomes $V^2$ where $V$ is the unigram vocabulary size. You will have to work with a high dimensional and sparse matrix.\n",
    "\n",
    "(iv) Using subword tokens instead of words: You will be able to capture the syntactic patterns of how words are combined or transformed. However this is not effective for the case where you want to capture the semantic characteristic of the tokens since creating subwords out of words produces bigger number of tokens. This means the dimesionality of the token matrix will be increased and be sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pointwise Mutual Information [20 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture, we introduced __pointwise mutual information__ (PMI), which addresses the issue of normalization removing information about absolute magnitudes of counts. The PMI for word $\\times$ context pair $(w,c)$ is \n",
    "\n",
    "$$\\log\\left(\\frac{P(w,c)}{P(w) \\cdot P(c)}\\right)$$\n",
    "\n",
    "with $\\log(0) = 0$. This is a measure of how far that cell's value deviates from what we would expect given the row and column sums for that cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a__. Implement `pmi`, a function which takes in a co-occurence matrix and returns the matrix with PMI normalization applied. [15 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(mat, positive=False):\n",
    "    \"\"\"Pointwise mutual information\n",
    "    \n",
    "    args:\n",
    "        - mat: 2d np.array to apply PMI\n",
    "        - positive: set to True for PPMI\n",
    "        \n",
    "    returns:\n",
    "        - pmi_mat: matrix of same shape with PMI applied\n",
    "    \"\"\"\n",
    "    def custom_log(num):\n",
    "        if not num:\n",
    "            return 0\n",
    "        else:\n",
    "            return np.log(num)\n",
    "        \n",
    "    pmi_mat = [[0 for _ in range(len(mat[0]))] for _ in range(len(mat))]\n",
    "    sum_w = np.reshape(np.sum(mat,axis=1), (mat.shape[0], 1))\n",
    "    sum_c = np.reshape(np.sum(mat,axis=0), (1, mat.shape[1]))\n",
    "    total_words = np.sum(sum_w)\n",
    "    for i in range(len(mat)):\n",
    "        for j in range(i, len(mat[0])):\n",
    "            numerator = mat[i][j] / total_words\n",
    "            denominator = (sum_w[i][0] / total_words) * (sum_c[0][j] / total_words)\n",
    "            val = custom_log(numerator / denominator)\n",
    "            if positive==True:\n",
    "                val = max(val, 0)\n",
    "            pmi_mat[i][j] = val\n",
    "            pmi_mat[j][i] = val\n",
    "    return pmi_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PMI to the co-occurence matrix computed above with `context_size=1`. What is the PMI between the words \"the\" and \"end\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PMI between the words \"the\" and \"end\": 1.72683\n"
     ]
    }
   ],
   "source": [
    "pmi_mat1 = pmi(mat1)\n",
    "\n",
    "print(\" PMI between the words \\\"the\\\" and \\\"end\\\": %.5f\" % pmi_mat1[tok2idx['the']][tok2idx['end']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b__. We also consider an extension of PMI, positive PMI (PPMI), that maps all negative PMI values to 0.0 ([Levy and Goldberg 2014](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization)). \n",
    "Write `ppmi`, which is the same as `pmi` except it applies PPMI instead of PMI (feel free to implement it as an option of `pmi`). What is the PMI of the words \"the\" and \"start\"? The PPMI? [5 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(mat, positive=False):\n",
    "    \"\"\"Pointwise mutual information\n",
    "    \n",
    "    args:\n",
    "        - mat: 2d np.array to apply PMI\n",
    "        - positive: set to True for PPMI\n",
    "        \n",
    "    returns:\n",
    "        - pmi_mat: matrix of same shape with PMI applied\n",
    "    \"\"\"\n",
    "    def custom_log(num):\n",
    "        if not num:\n",
    "            return 0\n",
    "        else:\n",
    "            return np.log(num)\n",
    "        \n",
    "    pmi_mat = [[0 for _ in range(len(mat[0]))] for _ in range(len(mat))]\n",
    "    sum_w = np.reshape(np.sum(mat,axis=1), (mat.shape[0], 1))\n",
    "    sum_c = np.reshape(np.sum(mat,axis=0), (1, mat.shape[1]))\n",
    "    total_words = np.sum(sum_w)\n",
    "    for i in range(len(mat)):\n",
    "        for j in range(i, len(mat[0])):\n",
    "            numerator = mat[i][j] / total_words\n",
    "            denominator = (sum_w[i][0] / total_words) * (sum_c[0][j] / total_words)\n",
    "            val = custom_log(numerator / denominator)\n",
    "            if positive==True:\n",
    "                val = max(val, 0)\n",
    "            pmi_mat[i][j] = val\n",
    "            pmi_mat[j][i] = val\n",
    "    return pmi_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_mat1 = pmi(mat1, False)\n",
    "ppmi_mat1 = pmi(mat1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI between the words \"the\" and \"start\": 0.292151278\n",
      "PPMI between the words \"the\" and \"start\": 0.292151278\n"
     ]
    }
   ],
   "source": [
    "print(\"PMI between the words \\\"the\\\" and \\\"start\\\": %.9f\" % pmi_mat1[tok2idx['the']][tok2idx['start']])\n",
    "print(\"PPMI between the words \\\"the\\\" and \\\"start\\\": %.9f\" % ppmi_mat1[tok2idx['the']][tok2idx['start']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing PMI [25 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a__. Consider the matrix `np.array([[1.0, 0.0, 0.0], [1000.0, 1000.0, 4000.0], [1000.0, 2000.0, 999.0]])`. Reweight this matrix using `ppmi`. (i) What is the value obtained for cell `[0,0]`, and (ii) give a brief description for what is likely problematic about this value. [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.6089380373924496, 0, 0],\n",
       " [0, 0, 0.28788209245444807],\n",
       " [0, 0.28788209245444807, 0]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmi(np.array([[1.0, 0.0, 0.0], [1000.0, 1000.0, 4000.0], [1000.0, 2000.0, 999.0]]), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) The value for cell [0,0] is 1.6089380373924496.\n",
    "\n",
    "(ii) The value of cell [0,0] is shown as a much higher value compared to any other values in the cell even though the the value of [0,0] in the original matrix is significantly lower than other values. Low frequency values should be considered with less weight when computing the PMI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b__. Give a suggestion for dealing with the problematic value and explain why it deals with this. Demonstrate your suggestion empirically [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the Normalized PMI.\n",
    "\n",
    "The Normalized PMI(NPMI)  for word $\\times$ context pair $(w,c)$ is \n",
    "\n",
    "$$\\log\\left(\\frac{P(w,c)}{P(w) \\cdot P(c)}\\right)\\bigg/{-\\log{P(w,c)}}$$\n",
    "\n",
    "This takes into account the co-occurence frequency of the word pairs. Word pairs $w, c$ that have low co-occurence frequency has small $P(w,c)$ $\\Rightarrow$ small  $\\log{P(w,c)}$ $\\Rightarrow$ big  $-\\log{P(w,c)}$ $\\Rightarrow$ small  $-1 / \\log{P(w,c)}$\n",
    "\n",
    "Therefore multiplying PMI by $-1 / \\log{P(w,c)}$ adds co-occurence frequency factor to the PMI and deals the problem we faced above.\n",
    "\n",
    "The value of NPMI can be viewed as follows.\n",
    "\n",
    "1) $w$ and $c$ never co-occur \n",
    "\n",
    "$$P(w,c) = 0 \\Rightarrow \\log{P(w,c)} \\rightarrow -\\infty \\Rightarrow NPMI = -1$$\n",
    "\n",
    "2) $w$ and $c$ are independent\n",
    "\n",
    "$$P(w,c) = P(w)P(c) \\Rightarrow PMI = 0 \\rightarrow NPMI = 0$$\n",
    "\n",
    "3) $w$ and $c$ always co-occurs\n",
    "\n",
    "$$P(w,c) = P(w) = P(c) \\Rightarrow NPMI = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a normalized PMI\n",
    "\n",
    "def npmi(mat, positive=False):\n",
    "    \"\"\"Nomralized Pointwise mutual information\n",
    "    \n",
    "    args:\n",
    "        - mat: 2d np.array to apply NPMI\n",
    "        - positive: set to True for PPMI\n",
    "        \n",
    "    returns:\n",
    "        - npmi_mat: matrix of same shape with PMI applied\n",
    "    \"\"\"\n",
    "    def custom_log(num):\n",
    "        if not num:\n",
    "            return 0\n",
    "        else:\n",
    "            return np.log(num)\n",
    "        \n",
    "    npmi_mat = [[0 for _ in range(len(mat[0]))] for _ in range(len(mat))]\n",
    "    sum_w = np.reshape(np.sum(mat,axis=1), (mat.shape[0], 1))\n",
    "    sum_c = np.reshape(np.sum(mat,axis=0), (1, mat.shape[1]))\n",
    "    total_words = np.sum(sum_w)\n",
    "\n",
    "    for i in range(len(mat)):\n",
    "        for j in range(i, len(mat[0])):\n",
    "            numerator = mat[i][j] / total_words\n",
    "            denominator = (sum_w[i][0] / total_words) * (sum_c[0][j] / total_words)\n",
    "            _pmi = custom_log(numerator / denominator)\n",
    "            normalizer = (-1) * np.log(numerator)\n",
    "            val = _pmi / normalizer\n",
    "            if positive==True:\n",
    "                val = max(val, 0)\n",
    "            npmi_mat[i][j] = val\n",
    "            npmi_mat[j][i] = val\n",
    "    return npmi_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NPMI\n",
    "npmi(np.array([[1.0, 0.0, 0.0], [1000.0, 1000.0, 4000.0], [1000.0, 2000.0, 999.0]]), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive NPMI\n",
    "npmi(np.array([[1.0, 0.0, 0.0], [1000.0, 1000.0, 4000.0], [1000.0, 2000.0, 999.0]]), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c__. Consider starting with a word-word co-occurence matrix and applied PMI to this matrix. (i) Which of the following describe the resulting vectors: sparse, dense, high-dimensional, low-dimensional (ii) If you wanted the opposite style of representation, what could you do? [5 pts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) Sparse and high-dimensional.\n",
    "\n",
    "(ii) Use Latent Semantic Analysis to perform dimensionality reduction on the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Analogy Evaluation [25 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word analogies provide another kind of evaluation for distributed representations. Here, we are given three vectors A, B, and C, in the relationship\n",
    "\n",
    "_A is to B as C is to __ _\n",
    "\n",
    "and asked to identify the fourth that completes the analogy. These analogies are by and large substantially easier than the classic brain-teaser analogies that used to appear on tests like the SAT, but it's still an interesting, demanding\n",
    "task. \n",
    "\n",
    "The core idea is that we make predictions by creating the vector\n",
    "\n",
    "$$(A - B) + C$$ \n",
    "\n",
    "and then ranking all vectors based on their distance from this new vector, choosing the closest as our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a__. Implement the function `analogy_completion`. [9 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def analogy_completion(a, b, c, mat):\n",
    "    \"\"\"Compute ? in \n",
    "    a is to b as c is to ? \n",
    "    as the closest to (b-a) + c\n",
    "    \n",
    "    mat:\n",
    "        - a dictionary of words and their vector representations\n",
    "        - key: word\n",
    "        - value: vector representation of each word as an array\n",
    "    \"\"\"\n",
    "    for x in (a, b, c):\n",
    "        if x not in mat.keys():\n",
    "            raise ValueError('%s is not in this dataset' % x)\n",
    "    new_vec = mat[b] - mat[a] + mat[c]\n",
    "    # compute the cosine distance\n",
    "    distances = [(word, cosine(new_vec, mat[word])) for word in mat.keys() if word not in (a, b, c)]\n",
    "    return sorted(distances, key=itemgetter(1), reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b__. Our simple word embeddings likely won't perform well on this task. Let's instead look at some high quality pretrained word embeddings. Write code to load 300-dimensional [GloVe word embeddings](http://nlp.stanford.edu/data/glove.840B.300d.zip) trained on 840B tokens. Each line of the file is formatted as a word followed by 300 floats that make up its corresponding word embedding (all space delimited). The entries of GloVe word embeddings are not counts, but instead are learned via machine learning. Use your `analogy_completion` code to complete the following analogies using the GloVe word embeddings. [6 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(glove_file, n_vecs=20000):\n",
    "    \"\"\" \"\"\"\n",
    "    tok2vec = {}\n",
    "    with open(glove_file, 'r') as glove_fh:\n",
    "        for i, row in enumerate(glove_fh):\n",
    "            word, vec = row.split(' ', 1)\n",
    "            tok2vec[word] = np.array([float(n) for n in vec.split(' ')])\n",
    "            if i >= n_vecs:\n",
    "                break\n",
    "    return tok2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = \"HW1/glove.840B.300d.txt\"\n",
    "glove_vecs = load_glove(glove_file, n_vecs=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vecs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Beijing\" is to \"China\" as \"Paris\" is to ?\n",
    "analogy_completion(\"Beijing\", \"China\", \"Paris\", glove_vecs)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"gold\" is to \"first\" as \"silver\" is to ?\n",
    "analogy_completion(\"gold\", \"first\", \"silver\", glove_vecs)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Italian\" is to \"mozzarella\" as \"American\" is to ?\n",
    "analogy_completion(\"Italian\", \"mozzarella\", \"American\", glove_vecs)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('awesome', 0.50272776147317089),\n",
       " ('cool', 0.52646862649484039),\n",
       " ('enjoyable', 0.53702365147115905),\n",
       " ('cute', 0.54849439029975011),\n",
       " ('exciting', 0.55400816850583301)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"research\" is to \"fun\" as \"engineering\" is to ?\n",
    "analogy_completion(\"research\", \"fun\", \"engineering\", glove_vecs)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Let's get a more quantitative, aggregate sense of the quality of GloVe embeddings. Load the analogies from `gram6-nationality-adjective.txt` and evaluate GloVe embeddings. Report the mean reciprocal rank of the correct answer (the last word on each line) for each analogy. [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_evaluation(glove_vecs, test_file, verbose=False):\n",
    "    # Read the test data file\n",
    "    test_file = \"HW1/\" + test_file\n",
    "    test_data = [line.split() for line in open(test_file).read().splitlines()]\n",
    "    test_data = [x for x in test_data if set(x) <= set(glove_vecs.keys())]\n",
    "\n",
    "    result_dic = defaultdict(int)\n",
    "    ranks = []\n",
    "    \n",
    "    # a is to b as c is to d\n",
    "    for a, b, c, d in test_data:\n",
    "        predicted_tup = analogy_completion(a, b, c, glove_vecs)\n",
    "        predicted_words, cos_dis = zip(*predicted_tup)\n",
    "        result_dic[predicted_words[0]==d] += 1\n",
    "        ranks.append(1.0 / (predicted_words.index(d)+1))\n",
    "            \n",
    "    mrr = np.mean(ranks)\n",
    "    return (mrr, result_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.95335458482388313, defaultdict(int, {True: 219, False: 15}))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy_evaluation(glove_vecs, \"gram6-nationality-adjective.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
