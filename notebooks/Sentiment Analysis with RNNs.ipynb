{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Recurrent Neural Networks\n",
    "\n",
    "\n",
    "![top-twitter-emojis.jpg](https://blog.emojipedia.org/content/images/size/w2000/2018/01/top-twitter-emojis.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be building a recurrent neural network and train it to do sentiment classification!\n",
    "\n",
    "The full data loading and processing code is provided for you, as is the code for a vanilla Elman-RNN. You'll be completing the implementaion of an LSTM-RNN and then comparing results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "\n",
    "### Load data\n",
    "Make sure you've downloaded the Stanford Sentiment Treebank that was used in lab last week. You can find it [here](http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "sst_home = '../data/trees'\n",
    "\n",
    "# Let's do 2-way positive/negative classification instead of 5-way\n",
    "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "    # so labels of 0 and 1 in te 5-wayclassificaiton are 0 in the 2-way. 3 and 4 are 1, and 2 is none\n",
    "    # because we don't have a neautral class. \n",
    "\n",
    "PADDING = \"<PAD>\"\n",
    "UNKNOWN = \"<UNK>\"\n",
    "max_seq_length = 20\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            example['label'] = easy_label_map[int(line[1])]\n",
    "            if example['label'] is None:\n",
    "                continue\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels---we don't need those here\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')\n",
    "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
    "test_set = load_sst_data(sst_home + '/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Next, we'll extract the vocabulary from the data, index each token, and finally convert the sentences into lists of indexed tokens. We are also padding and truncating all sentences to be of length=20. (Why? Think about how to handle batching. This is not the only way to do it! This is just simple.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def tokenize(string):\n",
    "    return string.split()\n",
    "\n",
    "def build_dictionary(training_datasets):\n",
    "    \"\"\"\n",
    "    Extract vocabulary and build dictionary.\n",
    "    \"\"\"  \n",
    "    word_counter = collections.Counter()\n",
    "    for i, dataset in enumerate(training_datasets):\n",
    "        for example in dataset:\n",
    "            word_counter.update(tokenize(example['text']))\n",
    "        \n",
    "    vocabulary = set([word for word in word_counter])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
    "        \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "    return word_indices, len(vocabulary)\n",
    "\n",
    "def sentences_to_padded_index_sequences(word_indices, datasets):\n",
    "    \"\"\"\n",
    "    Annotate datasets with feature vectors. Adding right-sided padding. \n",
    "    \"\"\"\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['text_index_sequence'] = torch.zeros(max_seq_length)\n",
    "\n",
    "            token_sequence = tokenize(example['text'])\n",
    "            padding = max_seq_length - len(token_sequence)\n",
    "\n",
    "            for i in range(max_seq_length):\n",
    "                if i >= len(token_sequence):\n",
    "                    index = word_indices[PADDING]\n",
    "                    pass\n",
    "                else:\n",
    "                    if token_sequence[i] in word_indices:\n",
    "                        index = word_indices[token_sequence[i]]\n",
    "                    else:\n",
    "                        index = word_indices[UNKNOWN]\n",
    "                example['text_index_sequence'][i] = index\n",
    "\n",
    "            example['text_index_sequence'] = example['text_index_sequence'].long().view(1,-1)\n",
    "            example['label'] = torch.LongTensor([example['label']])\n",
    "\n",
    "\n",
    "word_to_ix, vocab_size = build_dictionary([training_set])\n",
    "sentences_to_padded_index_sequences(word_to_ix, [training_set, dev_set, test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training dataset: 6920\n",
      "\n",
      "First padded and indexified example in training data:\n",
      " {'label': tensor([0]), 'text': 'Yet another entry in the sentimental oh-those-wacky-Brits genre that was ushered in by The Full Monty and is still straining to produce another smash hit .', 'text_index_sequence': tensor([[ 5957,  6674, 10700,  2187,  2957,  7959, 12589, 10807, 14335,  5618,\n",
      "         13260,  2187, 11685, 10763,  2265,  6099, 15783, 10090,  5653,  4847]])}\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of training dataset:\", len(training_set))\n",
    "print(\"\\nFirst padded and indexified example in training data:\\n\", training_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Batchify data\n",
    "We're going to be doign mini-batch training. The following code makes data iterators and a batchifying function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the iterator we'll use during training. \n",
    "# It's a generator that gives you one batch at a time.\n",
    "def data_iter(source, batch_size):\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)   \n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        yield [source[index] for index in batch_indices]\n",
    "\n",
    "# This is the iterator we use when we're evaluating our model. \n",
    "# It gives a list of batches that you can then iterate through.\n",
    "def eval_iter(source, batch_size):\n",
    "    batches = []\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while start < dataset_size - batch_size:\n",
    "        start += batch_size\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    return batches\n",
    "\n",
    "# The following function gives batches of vectors and labels, \n",
    "# these are the inputs to your model and loss function\n",
    "def get_batch(batch):\n",
    "    vectors = []\n",
    "    labels = []\n",
    "    for dict in batch:\n",
    "        vectors.append(dict[\"text_index_sequence\"])\n",
    "        labels.append(dict[\"label\"])\n",
    "    return vectors, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We'll be looking at accuracy as our evlauation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function outputs the accuracy on the dataset, we will use it during training.\n",
    "def evaluate(model, data_iter, is_lstm):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(len(data_iter)):\n",
    "        vectors, labels = get_batch(data_iter[i])\n",
    "        vectors = torch.stack(vectors).squeeze()\n",
    "        labels = torch.stack(labels).squeeze()\n",
    "        \n",
    "        if is_lstm:\n",
    "            hidden, c_t = model.init_hidden()\n",
    "            output, hidden = model(vectors, hidden, c_t)\n",
    "        else:\n",
    "            hidden = model.init_hidden()\n",
    "            output, hidden = model(vectors, hidden)\n",
    "        \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "      \n",
    "    return correct / float(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elman-RNN\n",
    "\n",
    "Note that when you're actually building and using these models for research or application, you will never want to build from scratch like we are today. This is simply for demonstration! And because it's a very useul exercise to do these things from scratch at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmanRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, batch_size):\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding_size = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.inlinear = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.hid2logits = nn.Linear(hidden_size, output_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x_emb = self.embed(x)                \n",
    "        embs = torch.chunk(x_emb, x_emb.size()[1], 1)\n",
    "        \n",
    "        def step(emb, hid):\n",
    "            combined = torch.cat((hid, emb), 1)\n",
    "            hid = torch.tanh(self.inlinear(combined))\n",
    "            return hid\n",
    "\n",
    "        for i in range(len(embs)):\n",
    "            hidden = step(embs[i].squeeze(), hidden)\n",
    "        \n",
    "        output = self.hid2logits(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        h0 = torch.zeros(self.batch_size, self.hidden_size)\n",
    "        return h0\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        lin_layers = [self.inlinear, self.hid2logits]\n",
    "        em_layer = [self.embed]\n",
    "     \n",
    "        for layer in lin_layers+em_layer:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            if layer in lin_layers:\n",
    "                layer.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(batch_size, num_epochs, model, loss_, optim, training_iter, dev_iter, train_eval_iter, is_lstm=False):\n",
    "    step = 0\n",
    "    epoch = 0\n",
    "    total_batches = int(len(training_set) / batch_size)\n",
    "    while epoch <= num_epochs:\n",
    "        model.train()\n",
    "        vectors, labels = get_batch(next(training_iter)) \n",
    "        vectors = torch.stack(vectors).squeeze() # batch_size, seq_len\n",
    "        labels = torch.stack(labels).squeeze()\n",
    "    \n",
    "        model.zero_grad()\n",
    "        \n",
    "        if is_lstm:\n",
    "            #assert \"Not yet implemented.\"\n",
    "            hidden, cell_state = model.init_hidden()\n",
    "            output, hidden = model(vectors, hidden, cell_state)\n",
    "        else:\n",
    "            hidden = model.init_hidden()\n",
    "            output, hidden = model(vectors, hidden)\n",
    "\n",
    "        lossy = loss_(output, labels)\n",
    "        lossy.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optim.step()\n",
    "        \n",
    "        if step % total_batches == 0:\n",
    "            model.eval()\n",
    "            if epoch % 20 == 0:\n",
    "                print(\"Epoch %i; Step %i; Loss %f; Train acc: %f; Dev acc %f\" \n",
    "                      %(epoch, step, lossy.data[0],\\\n",
    "                        evaluate(model, train_eval_iter, is_lstm),\\\n",
    "                        evaluate(model, dev_iter, is_lstm)))\n",
    "            epoch += 1\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model!\n",
    "\n",
    "We've provided the hyperparmaters you should use. We're also only evaluating on a part of the dev set to speed things along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0; Step 0; Loss 0.693210; Train acc: 0.519531; Dev acc 0.511719\n",
      "Epoch 20; Step 540; Loss 0.001854; Train acc: 1.000000; Dev acc 0.742188\n",
      "Epoch 40; Step 1080; Loss 0.000520; Train acc: 1.000000; Dev acc 0.738281\n",
      "Epoch 60; Step 1620; Loss 0.000450; Train acc: 1.000000; Dev acc 0.726562\n",
      "Epoch 80; Step 2160; Loss 0.000089; Train acc: 1.000000; Dev acc 0.738281\n",
      "Epoch 100; Step 2700; Loss 0.000032; Train acc: 1.000000; Dev acc 0.726562\n",
      "Epoch 120; Step 3240; Loss 0.000021; Train acc: 1.000000; Dev acc 0.726562\n",
      "Epoch 140; Step 3780; Loss 0.000015; Train acc: 1.000000; Dev acc 0.726562\n",
      "Epoch 160; Step 4320; Loss 0.000009; Train acc: 1.000000; Dev acc 0.726562\n",
      "Epoch 180; Step 4860; Loss 0.000007; Train acc: 1.000000; Dev acc 0.726562\n",
      "Epoch 200; Step 5400; Loss 0.000005; Train acc: 1.000000; Dev acc 0.722656\n",
      "Epoch 220; Step 5940; Loss 0.000003; Train acc: 1.000000; Dev acc 0.722656\n",
      "Epoch 240; Step 6480; Loss 0.000003; Train acc: 1.000000; Dev acc 0.722656\n",
      "Epoch 260; Step 7020; Loss 0.000002; Train acc: 1.000000; Dev acc 0.722656\n",
      "Epoch 280; Step 7560; Loss 0.000001; Train acc: 1.000000; Dev acc 0.722656\n",
      "Epoch 300; Step 8100; Loss 0.000001; Train acc: 1.000000; Dev acc 0.718750\n",
      "Epoch 320; Step 8640; Loss 0.000001; Train acc: 1.000000; Dev acc 0.718750\n",
      "Epoch 340; Step 9180; Loss 0.000001; Train acc: 1.000000; Dev acc 0.718750\n",
      "Epoch 360; Step 9720; Loss 0.000000; Train acc: 1.000000; Dev acc 0.710938\n",
      "Epoch 380; Step 10260; Loss 0.000000; Train acc: 1.000000; Dev acc 0.707031\n",
      "Epoch 400; Step 10800; Loss 0.000000; Train acc: 1.000000; Dev acc 0.707031\n",
      "Epoch 420; Step 11340; Loss 0.000000; Train acc: 1.000000; Dev acc 0.707031\n",
      "Epoch 440; Step 11880; Loss 0.000000; Train acc: 1.000000; Dev acc 0.707031\n",
      "Epoch 460; Step 12420; Loss 0.000000; Train acc: 1.000000; Dev acc 0.710938\n",
      "Epoch 480; Step 12960; Loss 0.000000; Train acc: 1.000000; Dev acc 0.710938\n",
      "Epoch 500; Step 13500; Loss 0.000000; Train acc: 1.000000; Dev acc 0.707031\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters \n",
    "input_size = vocab_size\n",
    "num_labels = 2 \n",
    "hidden_dim = 24\n",
    "embedding_dim = 8\n",
    "batch_size = 256\n",
    "learning_rate = 0.2\n",
    "#learning_rate = 0.0004\n",
    "num_epochs = 500\n",
    "\n",
    "\n",
    "\n",
    "# Build, initialize, and train model\n",
    "rnn = ElmanRNN(vocab_size, embedding_dim, hidden_dim, num_labels, batch_size)\n",
    "rnn.init_weights()\n",
    "\n",
    "# Loss and Optimizer\n",
    "loss = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "training_iter = data_iter(training_set, batch_size)\n",
    "train_eval_iter = eval_iter(training_set[:500], batch_size)\n",
    "dev_iter = eval_iter(dev_set[:500], batch_size)\n",
    "\n",
    "training_loop(batch_size, num_epochs, rnn, loss, optimizer, training_iter, dev_iter, train_eval_iter, is_lstm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## ☆ Implement LSTM! ☆\n",
    "\n",
    "Now we'll implement and LSTM-RNN! For a quick refresher on LSTMs, have a look at [Olah's blog-post](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "_This bears repeating: you should never actually implement modules like LSTM from scratch if the library has a clean, optimized implementation of it (as pytorch does)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x): \n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, batch_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding_size = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "               \n",
    "        \"\"\"\n",
    "        YOUR CODE GOES HERE\n",
    "        \n",
    "        \"\"\" \n",
    "        self.sig1 = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.sig2 = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.sig3 = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.tan1 = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        \n",
    "        \n",
    "        self.hid2logits = nn.Linear(hidden_size, output_size)\n",
    "        self.init_weights()\n",
    "        \n",
    "    \n",
    "    \n",
    "    def rand_arr(self, a, b, *args): \n",
    "        np.random.seed(0)\n",
    "        return np.random.rand(*args) * (b - a) + a\n",
    "    \n",
    "    def forward(self, x, hidden, c):\n",
    "        x_emb = self.embed(x)\n",
    "        embs = torch.chunk(x_emb, x_emb.size()[1], 1)       \n",
    "        \n",
    "        \"\"\"\n",
    "        YOUR CODE GOES HERE\n",
    "        \"\"\"    \n",
    "        def step(emb, hid, c):\n",
    "            combined = torch.cat((hid, emb), 1)\n",
    "            \n",
    "            sig1 = torch.sigmoid(self.sig1(combined))\n",
    "            sig2 = torch.sigmoid(self.sig2(combined))\n",
    "            sig3 = torch.sigmoid(self.sig3(combined))\n",
    "            tan1 = torch.tanh(self.tan1(combined))\n",
    "            \n",
    "            x1 = torch.mul(c, sig1)\n",
    "            x2 = torch.mul(sig2, tan1)\n",
    "            c = x1 + x2\n",
    "            hid = torch.mul(sig3, torch.tanh(c))\n",
    "            \n",
    "            return hid, c\n",
    "\n",
    "        for i in range(len(embs)):\n",
    "            hidden, c = step(embs[i].squeeze(), hidden, c)\n",
    "        \n",
    "        \n",
    "\n",
    "        output = self.hid2logits(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        h0 = torch.zeros(self.batch_size, self.hidden_size)\n",
    "        c0 = torch.zeros(self.batch_size, self.hidden_size)\n",
    "        return h0, c0\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        # The following is a placeholder, you need to add some code.\n",
    "        lin_layers = [self.hid2logits]\n",
    "        em_layer = [self.embed]\n",
    "     \n",
    "        for layer in lin_layers+em_layer:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            if layer in lin_layers:\n",
    "                layer.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM,\n",
    "\n",
    "Let's train the LSTM-RNN and see how performance compares with the Elman-RNN,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0; Step 0; Loss 0.694011; Train acc: 0.531250; Dev acc 0.531250\n",
      "Epoch 20; Step 540; Loss 0.692793; Train acc: 0.531250; Dev acc 0.531250\n",
      "Epoch 40; Step 1080; Loss 0.692907; Train acc: 0.531250; Dev acc 0.527344\n",
      "Epoch 60; Step 1620; Loss 0.700164; Train acc: 0.515625; Dev acc 0.496094\n",
      "Epoch 80; Step 2160; Loss 0.615319; Train acc: 0.675781; Dev acc 0.550781\n",
      "Epoch 100; Step 2700; Loss 0.446279; Train acc: 0.800781; Dev acc 0.605469\n",
      "Epoch 120; Step 3240; Loss 0.035270; Train acc: 0.980469; Dev acc 0.703125\n",
      "Epoch 140; Step 3780; Loss 0.049518; Train acc: 0.996094; Dev acc 0.742188\n",
      "Epoch 160; Step 4320; Loss 0.017907; Train acc: 0.996094; Dev acc 0.746094\n",
      "Epoch 180; Step 4860; Loss 0.001254; Train acc: 0.996094; Dev acc 0.730469\n",
      "Epoch 200; Step 5400; Loss 0.022535; Train acc: 0.996094; Dev acc 0.726562\n",
      "Epoch 220; Step 5940; Loss 0.001470; Train acc: 0.996094; Dev acc 0.714844\n",
      "Epoch 240; Step 6480; Loss 0.000775; Train acc: 1.000000; Dev acc 0.707031\n",
      "Epoch 260; Step 7020; Loss 0.000370; Train acc: 1.000000; Dev acc 0.695312\n",
      "Epoch 280; Step 7560; Loss 0.000175; Train acc: 1.000000; Dev acc 0.691406\n",
      "Epoch 300; Step 8100; Loss 0.000175; Train acc: 1.000000; Dev acc 0.699219\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters \n",
    "input_size = vocab_size\n",
    "num_labels = 2\n",
    "hidden_dim = 24\n",
    "embedding_dim = 8\n",
    "batch_size = 256\n",
    "learning_rate = 1.5\n",
    "num_epochs = 300\n",
    "\n",
    "\n",
    "# Build, initialize, and train model\n",
    "lstm = LSTM(vocab_size, embedding_dim, hidden_dim, num_labels, batch_size)\n",
    "lstm.init_weights()\n",
    "\n",
    "# Loss and Optimizer\n",
    "loss = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "training_iter = data_iter(training_set, batch_size)\n",
    "train_eval_iter = eval_iter(training_set[0:500], batch_size)\n",
    "dev_iter = eval_iter(dev_set[:500], batch_size)\n",
    "\n",
    "training_loop(batch_size, num_epochs, lstm, loss, optimizer, training_iter, dev_iter, train_eval_iter, is_lstm=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
