{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2: Deep Bag-of-Words\n",
    "\n",
    "![words.jpeg](https://cdn-images-1.medium.com/max/1600/0*JpqZhCNsQ_OGaRkB.jpg)\n",
    "\n",
    "<br>\n",
    "\n",
    "In this homework, you will be implementing a deep averaging network, detailed in [Deep Unordered Composition  Rivals Syntactic Methods for Text Classification by Iyyer et al. (2015)](https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf) and training it to do sentiment analysis on the Stanford Sentiment Treebank.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Please use all of the starter code that is provided, do not make any changes to the data processing, evaluation, and training functions. Only add code were you're asked to.**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Read Paper!\n",
    "\n",
    "Read [Deep Unordered Composition  Rivals Syntactic Methods for Text Classification by Iyyer et al. (2015)](https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "Make sure you've downloaded the Stanford Sentiment Treebank that was used in lab. You can find it [here](http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "random.seed(1)\n",
    "sst_home = '../data/trees'\n",
    "\n",
    "# Let's do 2-way positive/negative classification instead of 5-way\n",
    "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "    # so labels of 0 and 1 in te 5-wayclassificaiton are 0 in the 2-way. 3 and 4 are 1, and 2 is none\n",
    "    # because we don't have a neautral class. \n",
    "\n",
    "PADDING = \"<PAD>\"\n",
    "UNKNOWN = \"<UNK>\"\n",
    "max_seq_length = 20\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            example['label'] = easy_label_map[int(line[1])]\n",
    "            if example['label'] is None:\n",
    "                continue\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels---we don't need those here\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')\n",
    "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
    "test_set = load_sst_data(sst_home + '/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Next, we'll extract the vocabulary from the data, index each token, and finally convert the sentences into lists of indexed tokens. We are also padding and truncating all sentences to be of length=20. (Why? Think about how to handle batching. This is certainly not the only way! This is just simple.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def tokenize(string):\n",
    "    return string.split()\n",
    "\n",
    "def build_dictionary(training_datasets):\n",
    "    \"\"\"\n",
    "    Extract vocabulary and build dictionary.\n",
    "    \"\"\"  \n",
    "    word_counter = collections.Counter()\n",
    "    for i, dataset in enumerate(training_datasets):\n",
    "        for example in dataset:\n",
    "            word_counter.update(tokenize(example['text']))\n",
    "        \n",
    "    vocabulary = set([word for word in word_counter])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
    "        \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "    return word_indices, len(vocabulary)\n",
    "\n",
    "def sentences_to_padded_index_sequences(word_indices, datasets):\n",
    "    \"\"\"\n",
    "    Annotate datasets with feature vectors. Adding right-sided padding. \n",
    "    \"\"\"\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['text_index_sequence'] = torch.zeros(max_seq_length)\n",
    "\n",
    "            token_sequence = tokenize(example['text'])\n",
    "            padding = max_seq_length - len(token_sequence)\n",
    "\n",
    "            for i in range(max_seq_length):\n",
    "                if i >= len(token_sequence):\n",
    "                    index = word_indices[PADDING]\n",
    "                    pass\n",
    "                else:\n",
    "                    if token_sequence[i] in word_indices:\n",
    "                        index = word_indices[token_sequence[i]]\n",
    "                    else:\n",
    "                        index = word_indices[UNKNOWN]\n",
    "                example['text_index_sequence'][i] = index\n",
    "\n",
    "            example['text_index_sequence'] = example['text_index_sequence'].long().view(1,-1)\n",
    "            example['label'] = torch.LongTensor([example['label']])\n",
    "\n",
    "\n",
    "word_to_ix, vocab_size = build_dictionary([training_set])\n",
    "sentences_to_padded_index_sequences(word_to_ix, [training_set, dev_set, test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training dataset: 6920\n",
      "\n",
      "First padded and indexified example in training data:\n",
      " {'label': tensor([0]), 'text': 'Yet another entry in the sentimental oh-those-wacky-Brits genre that was ushered in by The Full Monty and is still straining to produce another smash hit .', 'text_index_sequence': tensor([[ 6630, 14958,  5529, 11731, 10404,  6018,  5996,  3578,  1604, 15477,\n",
      "         15994, 11731,  2252, 14478, 14779, 15470, 13751,  2930,  9065,  3418]])}\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of training dataset:\", len(training_set))\n",
    "print(\"\\nFirst padded and indexified example in training data:\\n\", training_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batichify data\n",
    "We're going to be doign mini-batch training. The following code makes data iterators and a batchifying function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the iterator we'll use during training. \n",
    "# It's a generator that gives you one batch at a time.\n",
    "def data_iter(source, batch_size):\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)   \n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        yield [source[index] for index in batch_indices]\n",
    "\n",
    "# This is the iterator we use when we're evaluating our model. \n",
    "# It gives a list of batches that you can then iterate through.\n",
    "def eval_iter(source, batch_size):\n",
    "    batches = []\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while start < dataset_size - batch_size:\n",
    "        start += batch_size\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    return batches\n",
    "\n",
    "# The following function gives batches of vectors and labels, \n",
    "# these are the inputs to your model and loss function\n",
    "def get_batch(batch):\n",
    "    vectors = []\n",
    "    labels = []\n",
    "    for dict in batch:\n",
    "        vectors.append(dict[\"text_index_sequence\"])\n",
    "        labels.append(dict[\"label\"])\n",
    "    return vectors, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We'll be looking at accuracy as our evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function outputs the accuracy on the dataset, we will use it during training.\n",
    "def evaluate(model, data_iter):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(len(data_iter)):\n",
    "        vectors, labels = get_batch(data_iter[i])\n",
    "        vectors = torch.stack(vectors).squeeze()\n",
    "        labels = torch.stack(labels).squeeze()\n",
    "\n",
    "        output = model(vectors)\n",
    "        \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "      \n",
    "    return correct / float(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Training Loop\n",
    "\n",
    "The following function trains the model and reports model accuracy on the train and dev set every 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(batch_size, num_epochs, model, loss_, optim, training_iter, dev_iter,\n",
    "                  train_eval_iter, verbose=True):\n",
    "    step = 0\n",
    "    epoch = 0\n",
    "    total_batches = int(len(training_set) / batch_size)\n",
    "    accuracies = []\n",
    "    while epoch <= num_epochs:\n",
    "        model.train()\n",
    "        vectors, labels = get_batch(next(training_iter)) \n",
    "        vectors = torch.stack(vectors).squeeze() # batch_size, seq_len\n",
    "        labels = torch.stack(labels).squeeze()\n",
    "        \n",
    "    \n",
    "        model.zero_grad()\n",
    "        \n",
    "        output = model(vectors)\n",
    "\n",
    "        lossy = loss_(output, labels)\n",
    "        lossy.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optim.step()\n",
    "        \n",
    "\n",
    "        if step % total_batches == 0:\n",
    "            model.eval()\n",
    "            if epoch % 5 == 0:\n",
    "                \n",
    "                train_acc = evaluate(model, train_eval_iter)\n",
    "                eval_acc = evaluate(model, dev_iter)\n",
    "                accuracies.append(eval_acc)\n",
    "                if verbose:\n",
    "                    print(\"Epoch %i; Step %i; Loss %f; Train acc: %f; Dev acc %f\" \n",
    "                          %(epoch, step, lossy.item(),\\\n",
    "                            train_acc, eval_acc))\n",
    "            epoch += 1\n",
    "        step += 1\n",
    "    \n",
    "    best_dev = max(accuracies)\n",
    "    print(\"Best dev accuracy is {}\".format(best_dev))\n",
    "    return best_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Part 1: Implement DAN (40 points)\n",
    "\n",
    "Following the [paper](https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf), implement the Deep Averaging Network (DAN).\n",
    "\n",
    "Implementation details,\n",
    "- Instead of using \\code{tanh} activations however, use \\code{ReLU}. \n",
    "- Make the number of layers a variable, not a fixed value.\n",
    "- Make sure to implement word-dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, \n",
    "                 batch_size, n_layers, drop_rate):\n",
    "        super(DAN, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding_size = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_rate = drop_rate\n",
    "        \n",
    "        \"\"\"\n",
    "        YOUR CODE GOES HERE\n",
    "        \"\"\"\n",
    "        \n",
    "        self.layer_0 = nn.Linear(embedding_dim, hidden_size)\n",
    "        \n",
    "        for i in range(1, self.n_layers):\n",
    "            setattr(self, 'l{}'.format(i), nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        YOUR CODE GOES HERE\n",
    "        \"\"\"\n",
    "        x_emb = self.embed(input)   # Get embedding of each word\n",
    "\n",
    "        if self.training:           # Dropout words in training step\n",
    "            av = []\n",
    "            for sent in x_emb:\n",
    "                new_sent = []\n",
    "                mask = np.random.rand(len(sent)) > drop_rate\n",
    "                \n",
    "                for index, keep in enumerate(mask):\n",
    "                    if keep:\n",
    "                        new_sent.append(sent[index])\n",
    "                \n",
    "                # Each sentence should at least have one word\n",
    "                if not new_sent:\n",
    "                    \n",
    "                    for j in range(len(sent)):\n",
    "                        new_sent.append(sent[j])\n",
    "                    set_trace()\n",
    "\n",
    "                new_sent = torch.stack(new_sent, 0)\n",
    "                av.append(torch.mean(new_sent, 0))    # append the average embedding to av\n",
    "                \n",
    "            av = torch.stack(av, 0)\n",
    "        \n",
    "        else:\n",
    "            av = torch.mean(x_emb, 1)\n",
    "        \n",
    "        h = self.layer_0(av)\n",
    "        lin_layers = []\n",
    "        for i in range(1, self.n_layers):\n",
    "            lin_layers.append(getattr(self, 'l{}'.format(i)))\n",
    "        \n",
    "        for layer in lin_layers:\n",
    "            h = layer(h)\n",
    "        \n",
    "        output = torch.softmax(self.decoder(h),1)\n",
    "        \"\"\"\n",
    "        \"\"\"        \n",
    "        return output\n",
    "\n",
    "    def init_hidden(self):\n",
    "        h0 = torch.zeros(self.batch_size, self.hidden_size)\n",
    "        return h0\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        lin_layers = [self.layer_0, self.decoder]\n",
    "        for i in range(1, self.n_layers):\n",
    "            lin_layers.append(getattr(self, 'l{}'.format(i)))\n",
    "        em_layer =  [self.embed]\n",
    "     \n",
    "        for layer in lin_layers+em_layer:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            if layer in lin_layers:\n",
    "                layer.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model!\n",
    "\n",
    "** Please use the hyperparameters and optimizer provided below. Do not make changes here. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0; Step 0; Loss 0.693148; Train acc: 0.460938; Dev acc 0.468750\n",
      "Epoch 5; Step 135; Loss 0.619379; Train acc: 0.742188; Dev acc 0.695312\n",
      "Epoch 10; Step 270; Loss 0.417765; Train acc: 0.910156; Dev acc 0.796875\n",
      "Epoch 15; Step 405; Loss 0.414722; Train acc: 0.949219; Dev acc 0.789062\n",
      "Epoch 20; Step 540; Loss 0.385289; Train acc: 0.964844; Dev acc 0.812500\n",
      "Epoch 25; Step 675; Loss 0.372346; Train acc: 0.980469; Dev acc 0.828125\n",
      "Epoch 30; Step 810; Loss 0.361986; Train acc: 0.988281; Dev acc 0.820312\n",
      "Best dev accuracy is 0.828125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.828125"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters \n",
    "input_size = vocab_size\n",
    "num_labels = 2\n",
    "hidden_dim = 24\n",
    "embedding_dim = 8\n",
    "batch_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "drop_rate = 0.4\n",
    "num_epochs = 30\n",
    "\n",
    "\n",
    "# Build and initialize the model\n",
    "dan = DAN(vocab_size, embedding_dim, hidden_dim, num_labels, batch_size, num_layers, drop_rate)\n",
    "dan.init_weights()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(dan.parameters(), lr=learning_rate)\n",
    "\n",
    "# Build data iterators\n",
    "training_iter = data_iter(training_set, batch_size)\n",
    "train_eval_iter = eval_iter(training_set[0:500], batch_size)\n",
    "dev_iter = eval_iter(dev_set[:500], batch_size)\n",
    "\n",
    "# Train the model\n",
    "training_loop(batch_size, num_epochs, dan, loss, optimizer, training_iter, dev_iter, train_eval_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Part 2: Hyperparameter tuning (40 points)\n",
    "\n",
    "Tune the DAN for learning rate, number of layers, and drop-out rate. Select a range for each parameter and then do a random search over these hyperparameters, trying a minimum 5 permutations of hyperparameters. Report results and the best hyperparameters you found. Do you see any patterns in your results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Select a range for each hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_size = vocab_size\n",
    "num_labels = 2\n",
    "hidden_dim = 24\n",
    "embedding_dim = 8\n",
    "batch_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "drop_rate = 0.4\n",
    "max_seq_length = 20\n",
    "num_epochs = 30\n",
    "\n",
    "param_dic = {\"learning_rate\" : [0.0008, 0.0009, 0.001, 0.0011, 0.0012, 0.002, 0.003], \n",
    "             \"num_layers\" : [1, 2, 3, 4, 5, 6], \n",
    "             \"drop_rate\" : [0, 0.2, 0.3, 0.4, 0.45]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1. learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Search 1) learning_rate = 0.0008\n",
      "Epoch 0; Step 0; Loss 0.693139; Train acc: 0.507812; Dev acc 0.535156\n",
      "Epoch 5; Step 135; Loss 0.658259; Train acc: 0.656250; Dev acc 0.621094\n",
      "Epoch 10; Step 270; Loss 0.466037; Train acc: 0.851562; Dev acc 0.765625\n",
      "Epoch 15; Step 405; Loss 0.440024; Train acc: 0.949219; Dev acc 0.792969\n",
      "Epoch 20; Step 540; Loss 0.397193; Train acc: 0.968750; Dev acc 0.785156\n",
      "Epoch 25; Step 675; Loss 0.396816; Train acc: 0.984375; Dev acc 0.789062\n",
      "Epoch 30; Step 810; Loss 0.377116; Train acc: 0.984375; Dev acc 0.789062\n",
      "Best dev accuracy is 0.79296875\n",
      "\n",
      "(Search 2) learning_rate = 0.0009\n",
      "Epoch 0; Step 0; Loss 0.693152; Train acc: 0.484375; Dev acc 0.500000\n",
      "Epoch 5; Step 135; Loss 0.631062; Train acc: 0.746094; Dev acc 0.644531\n",
      "Epoch 10; Step 270; Loss 0.465599; Train acc: 0.914062; Dev acc 0.761719\n",
      "Epoch 15; Step 405; Loss 0.422594; Train acc: 0.968750; Dev acc 0.800781\n",
      "Epoch 20; Step 540; Loss 0.396941; Train acc: 0.976562; Dev acc 0.835938\n",
      "Epoch 25; Step 675; Loss 0.366876; Train acc: 0.980469; Dev acc 0.820312\n",
      "Epoch 30; Step 810; Loss 0.366215; Train acc: 0.976562; Dev acc 0.828125\n",
      "Best dev accuracy is 0.8359375\n",
      "\n",
      "(Search 3) learning_rate = 0.001\n",
      "Epoch 0; Step 0; Loss 0.693146; Train acc: 0.539062; Dev acc 0.484375\n",
      "Epoch 5; Step 135; Loss 0.609649; Train acc: 0.718750; Dev acc 0.746094\n",
      "Epoch 10; Step 270; Loss 0.422477; Train acc: 0.906250; Dev acc 0.847656\n",
      "Epoch 15; Step 405; Loss 0.377809; Train acc: 0.953125; Dev acc 0.828125\n",
      "Epoch 20; Step 540; Loss 0.376998; Train acc: 0.957031; Dev acc 0.812500\n",
      "Epoch 25; Step 675; Loss 0.416727; Train acc: 0.972656; Dev acc 0.820312\n",
      "Epoch 30; Step 810; Loss 0.386992; Train acc: 0.972656; Dev acc 0.808594\n",
      "Best dev accuracy is 0.84765625\n",
      "\n",
      "(Search 4) learning_rate = 0.0011\n",
      "Epoch 0; Step 0; Loss 0.693188; Train acc: 0.488281; Dev acc 0.535156\n",
      "Epoch 5; Step 135; Loss 0.559780; Train acc: 0.769531; Dev acc 0.726562\n",
      "Epoch 10; Step 270; Loss 0.438231; Train acc: 0.937500; Dev acc 0.789062\n",
      "Epoch 15; Step 405; Loss 0.382954; Train acc: 0.968750; Dev acc 0.781250\n",
      "Epoch 20; Step 540; Loss 0.388449; Train acc: 0.980469; Dev acc 0.773438\n",
      "Epoch 25; Step 675; Loss 0.375542; Train acc: 0.976562; Dev acc 0.750000\n",
      "Epoch 30; Step 810; Loss 0.344832; Train acc: 0.988281; Dev acc 0.761719\n",
      "Best dev accuracy is 0.7890625\n",
      "\n",
      "(Search 5) learning_rate = 0.0012\n",
      "Epoch 0; Step 0; Loss 0.693167; Train acc: 0.554688; Dev acc 0.550781\n",
      "Epoch 5; Step 135; Loss 0.546977; Train acc: 0.816406; Dev acc 0.742188\n",
      "Epoch 10; Step 270; Loss 0.404839; Train acc: 0.929688; Dev acc 0.820312\n",
      "Epoch 15; Step 405; Loss 0.391237; Train acc: 0.968750; Dev acc 0.812500\n",
      "Epoch 20; Step 540; Loss 0.389052; Train acc: 0.976562; Dev acc 0.812500\n",
      "Epoch 25; Step 675; Loss 0.361076; Train acc: 0.984375; Dev acc 0.800781\n",
      "Epoch 30; Step 810; Loss 0.386577; Train acc: 0.996094; Dev acc 0.777344\n",
      "Best dev accuracy is 0.8203125\n",
      "\n",
      "(Search 6) learning_rate = 0.002\n",
      "Epoch 0; Step 0; Loss 0.693150; Train acc: 0.496094; Dev acc 0.492188\n",
      "Epoch 5; Step 135; Loss 0.484471; Train acc: 0.921875; Dev acc 0.832031\n",
      "Epoch 10; Step 270; Loss 0.382736; Train acc: 0.976562; Dev acc 0.812500\n",
      "Epoch 15; Step 405; Loss 0.402161; Train acc: 0.984375; Dev acc 0.816406\n",
      "Epoch 20; Step 540; Loss 0.368713; Train acc: 0.992188; Dev acc 0.812500\n",
      "Epoch 25; Step 675; Loss 0.401863; Train acc: 0.988281; Dev acc 0.789062\n",
      "Epoch 30; Step 810; Loss 0.365411; Train acc: 0.996094; Dev acc 0.789062\n",
      "Best dev accuracy is 0.83203125\n",
      "\n",
      "(Search 7) learning_rate = 0.003\n",
      "Epoch 0; Step 0; Loss 0.693129; Train acc: 0.484375; Dev acc 0.441406\n",
      "Epoch 5; Step 135; Loss 0.424971; Train acc: 0.910156; Dev acc 0.792969\n",
      "Epoch 10; Step 270; Loss 0.377076; Train acc: 0.964844; Dev acc 0.773438\n",
      "Epoch 15; Step 405; Loss 0.405696; Train acc: 0.984375; Dev acc 0.750000\n",
      "Epoch 20; Step 540; Loss 0.404243; Train acc: 0.980469; Dev acc 0.742188\n",
      "Epoch 25; Step 675; Loss 0.382515; Train acc: 0.980469; Dev acc 0.746094\n",
      "Epoch 30; Step 810; Loss 0.382299; Train acc: 0.976562; Dev acc 0.742188\n",
      "Best dev accuracy is 0.79296875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "drop_rate = 0.4\n",
    "\n",
    "learning_rate_acc = []\n",
    "# Search learning_rate\n",
    "for i in range(len(param_dic[\"learning_rate\"])):\n",
    "    print(\"(Search %d) learning_rate = %s\" % (i+1, str(param_dic[\"learning_rate\"][i])))\n",
    "    learning_rate = param_dic[\"learning_rate\"][i]\n",
    "\n",
    "    # Build and initialize the model\n",
    "    dan = DAN(vocab_size, embedding_dim, hidden_dim, num_labels, batch_size, num_layers, drop_rate)\n",
    "    dan.init_weights()\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss = nn.CrossEntropyLoss()  \n",
    "    optimizer = torch.optim.Adam(dan.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Build data iterators\n",
    "    training_iter = data_iter(training_set, batch_size)\n",
    "    train_eval_iter = eval_iter(training_set[0:500], batch_size)\n",
    "    dev_iter = eval_iter(dev_set[:500], batch_size)\n",
    "\n",
    "    # Train the model\n",
    "    acc = training_loop(batch_size, num_epochs, dan, loss, optimizer, training_iter, dev_iter, train_eval_iter)\n",
    "    learning_rate_acc.append(acc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11fcf0a58>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(param_dic[\"learning_rate\"], learning_rate_acc)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlim(param_dic[\"learning_rate\"][0] - 0.0003, param_dic[\"learning_rate\"][-1] + 0.0003)\n",
    "for x, y in zip(param_dic[\"learning_rate\"], learning_rate_acc):\n",
    "    plt.text(x, y, str(round(y, 4)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pattern:__ The accuracy seems to be at high values(over 0.83) around learning rate [0.0008, 0.0012]. I will check this range in random search. The accuracy drops dramatically for learning rate higher than 0.002."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2. num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Search 1) num_layers = 1\n",
      "Epoch 0; Step 0; Loss 0.693091; Train acc: 0.527344; Dev acc 0.464844\n",
      "Epoch 5; Step 135; Loss 0.648788; Train acc: 0.675781; Dev acc 0.593750\n",
      "Epoch 10; Step 270; Loss 0.523492; Train acc: 0.878906; Dev acc 0.765625\n",
      "Epoch 15; Step 405; Loss 0.445006; Train acc: 0.937500; Dev acc 0.785156\n",
      "Epoch 20; Step 540; Loss 0.428633; Train acc: 0.964844; Dev acc 0.789062\n",
      "Epoch 25; Step 675; Loss 0.392082; Train acc: 0.984375; Dev acc 0.789062\n",
      "Epoch 30; Step 810; Loss 0.394962; Train acc: 0.984375; Dev acc 0.789062\n",
      "Best dev accuracy is 0.7890625\n",
      "\n",
      "(Search 2) num_layers = 2\n",
      "Epoch 0; Step 0; Loss 0.693154; Train acc: 0.527344; Dev acc 0.542969\n",
      "Epoch 5; Step 135; Loss 0.600979; Train acc: 0.789062; Dev acc 0.777344\n",
      "Epoch 10; Step 270; Loss 0.441554; Train acc: 0.902344; Dev acc 0.820312\n",
      "Epoch 15; Step 405; Loss 0.406136; Train acc: 0.941406; Dev acc 0.816406\n",
      "Epoch 20; Step 540; Loss 0.397388; Train acc: 0.964844; Dev acc 0.777344\n",
      "Epoch 25; Step 675; Loss 0.385184; Train acc: 0.968750; Dev acc 0.781250\n",
      "Epoch 30; Step 810; Loss 0.380847; Train acc: 0.972656; Dev acc 0.757812\n",
      "Best dev accuracy is 0.8203125\n",
      "\n",
      "(Search 3) num_layers = 3\n",
      "Epoch 0; Step 0; Loss 0.693147; Train acc: 0.531250; Dev acc 0.527344\n",
      "Epoch 5; Step 135; Loss 0.536900; Train acc: 0.777344; Dev acc 0.738281\n",
      "Epoch 10; Step 270; Loss 0.425164; Train acc: 0.945312; Dev acc 0.796875\n",
      "Epoch 15; Step 405; Loss 0.371797; Train acc: 0.968750; Dev acc 0.769531\n",
      "Epoch 20; Step 540; Loss 0.368055; Train acc: 0.976562; Dev acc 0.750000\n",
      "Epoch 25; Step 675; Loss 0.368828; Train acc: 0.984375; Dev acc 0.742188\n",
      "Epoch 30; Step 810; Loss 0.392930; Train acc: 0.992188; Dev acc 0.746094\n",
      "Best dev accuracy is 0.796875\n",
      "\n",
      "(Search 4) num_layers = 4\n",
      "Epoch 0; Step 0; Loss 0.693147; Train acc: 0.519531; Dev acc 0.523438\n",
      "Epoch 5; Step 135; Loss 0.513366; Train acc: 0.781250; Dev acc 0.707031\n",
      "Epoch 10; Step 270; Loss 0.425306; Train acc: 0.917969; Dev acc 0.750000\n",
      "Epoch 15; Step 405; Loss 0.396787; Train acc: 0.960938; Dev acc 0.769531\n",
      "Epoch 20; Step 540; Loss 0.384630; Train acc: 0.964844; Dev acc 0.761719\n",
      "Epoch 25; Step 675; Loss 0.380531; Train acc: 0.968750; Dev acc 0.769531\n",
      "Epoch 30; Step 810; Loss 0.393182; Train acc: 0.976562; Dev acc 0.765625\n",
      "Best dev accuracy is 0.76953125\n",
      "\n",
      "(Search 5) num_layers = 5\n",
      "Epoch 0; Step 0; Loss 0.693147; Train acc: 0.507812; Dev acc 0.460938\n",
      "Epoch 5; Step 135; Loss 0.558864; Train acc: 0.796875; Dev acc 0.679688\n",
      "Epoch 10; Step 270; Loss 0.395108; Train acc: 0.949219; Dev acc 0.781250\n",
      "Epoch 15; Step 405; Loss 0.390116; Train acc: 0.964844; Dev acc 0.765625\n",
      "Epoch 20; Step 540; Loss 0.401329; Train acc: 0.968750; Dev acc 0.750000\n",
      "Epoch 25; Step 675; Loss 0.387635; Train acc: 0.976562; Dev acc 0.742188\n",
      "Epoch 30; Step 810; Loss 0.359101; Train acc: 0.984375; Dev acc 0.753906\n",
      "Best dev accuracy is 0.78125\n",
      "\n",
      "(Search 6) num_layers = 6\n",
      "Epoch 0; Step 0; Loss 0.693146; Train acc: 0.488281; Dev acc 0.468750\n",
      "Epoch 5; Step 135; Loss 0.583910; Train acc: 0.707031; Dev acc 0.687500\n",
      "Epoch 10; Step 270; Loss 0.386803; Train acc: 0.925781; Dev acc 0.796875\n",
      "Epoch 15; Step 405; Loss 0.404629; Train acc: 0.960938; Dev acc 0.804688\n",
      "Epoch 20; Step 540; Loss 0.397734; Train acc: 0.964844; Dev acc 0.773438\n",
      "Epoch 25; Step 675; Loss 0.407590; Train acc: 0.968750; Dev acc 0.808594\n",
      "Epoch 30; Step 810; Loss 0.381689; Train acc: 0.972656; Dev acc 0.785156\n",
      "Best dev accuracy is 0.80859375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "drop_rate = 0.4\n",
    "\n",
    "num_layers_acc = []\n",
    "# Search num_layers\n",
    "for i in range(len(param_dic[\"num_layers\"])):\n",
    "    print(\"(Search %d) num_layers = %s\" % (i+1, str(param_dic[\"num_layers\"][i])))\n",
    "    num_layers = param_dic[\"num_layers\"][i]\n",
    "\n",
    "    # Build and initialize the model\n",
    "    dan = DAN(vocab_size, embedding_dim, hidden_dim, num_labels, batch_size, num_layers, drop_rate)\n",
    "    dan.init_weights()\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss = nn.CrossEntropyLoss()  \n",
    "    optimizer = torch.optim.Adam(dan.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Build data iterators\n",
    "    training_iter = data_iter(training_set, batch_size)\n",
    "    train_eval_iter = eval_iter(training_set[0:500], batch_size)\n",
    "    dev_iter = eval_iter(dev_set[:500], batch_size)\n",
    "\n",
    "    # Train the model\n",
    "    acc = training_loop(batch_size, num_epochs, dan, loss, optimizer, training_iter, dev_iter, train_eval_iter)\n",
    "    num_layers_acc.append(acc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f9f8fd0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(param_dic[\"num_layers\"], num_layers_acc)\n",
    "plt.xlabel('Number of Layers')\n",
    "plt.ylabel('Accuracy')\n",
    "for x, y in zip(param_dic[\"num_layers\"], num_layers_acc):\n",
    "    plt.text(x, y, str(round(y, 4)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pattern:__ The accuracy suddenly drops when the number of layers becomes 4 and more. This graph has a similar pattern with DAN paper's Figure 4, where the accuray increases until layer three and drops after 4. [1, 4] seems to be a good range to consider for random search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-3. drop_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Search 1) drop_rate = 0\n",
      "Epoch 0; Step 0; Loss 0.693165; Train acc: 0.500000; Dev acc 0.519531\n",
      "Epoch 5; Step 135; Loss 0.557901; Train acc: 0.839844; Dev acc 0.777344\n",
      "Epoch 10; Step 270; Loss 0.364223; Train acc: 0.976562; Dev acc 0.789062\n",
      "Epoch 15; Step 405; Loss 0.338267; Train acc: 0.992188; Dev acc 0.773438\n",
      "Epoch 20; Step 540; Loss 0.323144; Train acc: 0.996094; Dev acc 0.769531\n",
      "Epoch 25; Step 675; Loss 0.321985; Train acc: 0.996094; Dev acc 0.757812\n",
      "Epoch 30; Step 810; Loss 0.314224; Train acc: 0.996094; Dev acc 0.765625\n",
      "Best dev accuracy is 0.7890625\n",
      "\n",
      "(Search 2) drop_rate = 0.2\n",
      "Epoch 0; Step 0; Loss 0.693140; Train acc: 0.535156; Dev acc 0.550781\n",
      "Epoch 5; Step 135; Loss 0.565616; Train acc: 0.738281; Dev acc 0.710938\n",
      "Epoch 10; Step 270; Loss 0.408517; Train acc: 0.949219; Dev acc 0.792969\n",
      "Epoch 15; Step 405; Loss 0.349191; Train acc: 0.968750; Dev acc 0.796875\n",
      "Epoch 20; Step 540; Loss 0.338436; Train acc: 0.972656; Dev acc 0.804688\n",
      "Epoch 25; Step 675; Loss 0.331599; Train acc: 0.992188; Dev acc 0.789062\n",
      "Epoch 30; Step 810; Loss 0.341904; Train acc: 0.996094; Dev acc 0.769531\n",
      "Best dev accuracy is 0.8046875\n",
      "\n",
      "(Search 3) drop_rate = 0.3\n",
      "Epoch 0; Step 0; Loss 0.693156; Train acc: 0.523438; Dev acc 0.531250\n",
      "Epoch 5; Step 135; Loss 0.562661; Train acc: 0.781250; Dev acc 0.757812\n",
      "Epoch 10; Step 270; Loss 0.411171; Train acc: 0.937500; Dev acc 0.804688\n",
      "Epoch 15; Step 405; Loss 0.376891; Train acc: 0.949219; Dev acc 0.769531\n",
      "Epoch 20; Step 540; Loss 0.371588; Train acc: 0.968750; Dev acc 0.769531\n",
      "Epoch 25; Step 675; Loss 0.351919; Train acc: 0.980469; Dev acc 0.785156\n",
      "Epoch 30; Step 810; Loss 0.345566; Train acc: 0.988281; Dev acc 0.769531\n",
      "Best dev accuracy is 0.8046875\n",
      "\n",
      "(Search 4) drop_rate = 0.4\n",
      "Epoch 0; Step 0; Loss 0.693147; Train acc: 0.531250; Dev acc 0.550781\n",
      "Epoch 5; Step 135; Loss 0.587247; Train acc: 0.789062; Dev acc 0.683594\n",
      "Epoch 10; Step 270; Loss 0.430107; Train acc: 0.972656; Dev acc 0.781250\n",
      "Epoch 15; Step 405; Loss 0.425904; Train acc: 0.957031; Dev acc 0.769531\n",
      "Epoch 20; Step 540; Loss 0.391489; Train acc: 0.984375; Dev acc 0.792969\n",
      "Epoch 25; Step 675; Loss 0.392550; Train acc: 0.984375; Dev acc 0.773438\n",
      "Epoch 30; Step 810; Loss 0.372190; Train acc: 0.988281; Dev acc 0.777344\n",
      "Best dev accuracy is 0.79296875\n",
      "\n",
      "(Search 5) drop_rate = 0.45\n",
      "Epoch 0; Step 0; Loss 0.693133; Train acc: 0.511719; Dev acc 0.515625\n",
      "Epoch 5; Step 135; Loss 0.603163; Train acc: 0.734375; Dev acc 0.621094\n",
      "Epoch 10; Step 270; Loss 0.441864; Train acc: 0.882812; Dev acc 0.750000\n",
      "Epoch 15; Step 405; Loss 0.439317; Train acc: 0.953125; Dev acc 0.781250\n",
      "Epoch 20; Step 540; Loss 0.420474; Train acc: 0.957031; Dev acc 0.761719\n",
      "Epoch 25; Step 675; Loss 0.394026; Train acc: 0.976562; Dev acc 0.769531\n",
      "Epoch 30; Step 810; Loss 0.390779; Train acc: 0.984375; Dev acc 0.761719\n",
      "Best dev accuracy is 0.78125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "drop_rate = 0.4\n",
    "\n",
    "drop_rate_acc = []\n",
    "# Search drop_rate\n",
    "for i in range(len(param_dic[\"drop_rate\"])):\n",
    "    print(\"(Search %d) drop_rate = %s\" % (i+1, str(param_dic[\"drop_rate\"][i])))\n",
    "    drop_rate = param_dic[\"drop_rate\"][i]\n",
    "\n",
    "    # Build and initialize the model\n",
    "    dan = DAN(vocab_size, embedding_dim, hidden_dim, num_labels, batch_size, num_layers, drop_rate)\n",
    "    dan.init_weights()\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss = nn.CrossEntropyLoss()  \n",
    "    optimizer = torch.optim.Adam(dan.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Build data iterators\n",
    "    training_iter = data_iter(training_set, batch_size)\n",
    "    train_eval_iter = eval_iter(training_set[0:500], batch_size)\n",
    "    dev_iter = eval_iter(dev_set[:500], batch_size)\n",
    "\n",
    "    # Train the model\n",
    "    acc = training_loop(batch_size, num_epochs, dan, loss, optimizer, training_iter, dev_iter, train_eval_iter)\n",
    "    drop_rate_acc.append(acc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f8df710>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(param_dic[\"drop_rate\"], drop_rate_acc)\n",
    "plt.xlabel('Drop Rate')\n",
    "plt.ylabel('Accuracy')\n",
    "for x, y in zip(param_dic[\"drop_rate\"], drop_rate_acc):\n",
    "    plt.text(x, y, str(round(y, 4)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pattern:__ The accuracy falls at drop rate 0.4. The original paper for DAN shows a similar result on Figure 2 and mentions that drop rate of 0.3 was optimal for one of its tasks. I will look into the range [0.2, 0.4) for random search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Search 0)\n",
      "    learning_rate: 0.0012\n",
      "    num_layers: 4\n",
      "    drop_rate: 0.25\n",
      "Epoch 0; Step 0; Loss 0.693148; Train acc: 0.539062; Dev acc 0.539062\n",
      "Epoch 5; Step 135; Loss 0.492182; Train acc: 0.839844; Dev acc 0.746094\n",
      "Epoch 10; Step 270; Loss 0.398030; Train acc: 0.945312; Dev acc 0.820312\n",
      "Epoch 15; Step 405; Loss 0.362300; Train acc: 0.972656; Dev acc 0.792969\n",
      "Epoch 20; Step 540; Loss 0.356329; Train acc: 0.976562; Dev acc 0.796875\n",
      "Epoch 25; Step 675; Loss 0.362534; Train acc: 0.988281; Dev acc 0.800781\n",
      "Epoch 30; Step 810; Loss 0.341245; Train acc: 0.996094; Dev acc 0.792969\n",
      "Best dev accuracy is 0.8203125\n",
      "\n",
      "(Search 1)\n",
      "    learning_rate: 0.0010\n",
      "    num_layers: 1\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693100; Train acc: 0.476562; Dev acc 0.468750\n",
      "Epoch 5; Step 135; Loss 0.646240; Train acc: 0.746094; Dev acc 0.726562\n",
      "Epoch 10; Step 270; Loss 0.463922; Train acc: 0.882812; Dev acc 0.808594\n",
      "Epoch 15; Step 405; Loss 0.404402; Train acc: 0.957031; Dev acc 0.820312\n",
      "Epoch 20; Step 540; Loss 0.377966; Train acc: 0.972656; Dev acc 0.792969\n",
      "Epoch 25; Step 675; Loss 0.376495; Train acc: 0.976562; Dev acc 0.785156\n",
      "Epoch 30; Step 810; Loss 0.349418; Train acc: 0.976562; Dev acc 0.785156\n",
      "Best dev accuracy is 0.8203125\n",
      "\n",
      "(Search 2)\n",
      "    learning_rate: 0.0011\n",
      "    num_layers: 1\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693152; Train acc: 0.496094; Dev acc 0.527344\n",
      "Epoch 5; Step 135; Loss 0.642362; Train acc: 0.722656; Dev acc 0.722656\n",
      "Epoch 10; Step 270; Loss 0.463390; Train acc: 0.851562; Dev acc 0.804688\n",
      "Epoch 15; Step 405; Loss 0.416607; Train acc: 0.945312; Dev acc 0.816406\n",
      "Epoch 20; Step 540; Loss 0.371279; Train acc: 0.972656; Dev acc 0.839844\n",
      "Epoch 25; Step 675; Loss 0.353624; Train acc: 0.988281; Dev acc 0.832031\n",
      "Epoch 30; Step 810; Loss 0.350626; Train acc: 0.992188; Dev acc 0.824219\n",
      "Best dev accuracy is 0.83984375\n",
      "\n",
      "(Search 3)\n",
      "    learning_rate: 0.0008\n",
      "    num_layers: 2\n",
      "    drop_rate: 0.25\n",
      "Epoch 0; Step 0; Loss 0.693146; Train acc: 0.464844; Dev acc 0.484375\n",
      "Epoch 5; Step 135; Loss 0.629189; Train acc: 0.730469; Dev acc 0.695312\n",
      "Epoch 10; Step 270; Loss 0.436120; Train acc: 0.875000; Dev acc 0.785156\n",
      "Epoch 15; Step 405; Loss 0.372263; Train acc: 0.960938; Dev acc 0.828125\n",
      "Epoch 20; Step 540; Loss 0.355565; Train acc: 0.976562; Dev acc 0.792969\n",
      "Epoch 25; Step 675; Loss 0.362222; Train acc: 0.976562; Dev acc 0.800781\n",
      "Epoch 30; Step 810; Loss 0.351769; Train acc: 0.980469; Dev acc 0.812500\n",
      "Best dev accuracy is 0.828125\n",
      "\n",
      "(Search 4)\n",
      "    learning_rate: 0.0010\n",
      "    num_layers: 4\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693147; Train acc: 0.496094; Dev acc 0.531250\n",
      "Epoch 5; Step 135; Loss 0.456887; Train acc: 0.828125; Dev acc 0.730469\n",
      "Epoch 10; Step 270; Loss 0.368288; Train acc: 0.949219; Dev acc 0.773438\n",
      "Epoch 15; Step 405; Loss 0.353042; Train acc: 0.968750; Dev acc 0.773438\n",
      "Epoch 20; Step 540; Loss 0.353157; Train acc: 0.972656; Dev acc 0.777344\n",
      "Epoch 25; Step 675; Loss 0.369931; Train acc: 0.976562; Dev acc 0.765625\n",
      "Epoch 30; Step 810; Loss 0.340510; Train acc: 0.984375; Dev acc 0.757812\n",
      "Best dev accuracy is 0.77734375\n",
      "\n",
      "(Search 5)\n",
      "    learning_rate: 0.0010\n",
      "    num_layers: 3\n",
      "    drop_rate: 0.25\n",
      "Epoch 0; Step 0; Loss 0.693147; Train acc: 0.558594; Dev acc 0.492188\n",
      "Epoch 5; Step 135; Loss 0.514642; Train acc: 0.769531; Dev acc 0.742188\n",
      "Epoch 10; Step 270; Loss 0.389406; Train acc: 0.949219; Dev acc 0.785156\n",
      "Epoch 15; Step 405; Loss 0.340280; Train acc: 0.964844; Dev acc 0.765625\n",
      "Epoch 20; Step 540; Loss 0.352778; Train acc: 0.980469; Dev acc 0.750000\n",
      "Epoch 25; Step 675; Loss 0.366556; Train acc: 0.992188; Dev acc 0.746094\n",
      "Epoch 30; Step 810; Loss 0.350039; Train acc: 0.992188; Dev acc 0.750000\n",
      "Best dev accuracy is 0.78515625\n",
      "\n",
      "(Search 6)\n",
      "    learning_rate: 0.0011\n",
      "    num_layers: 1\n",
      "    drop_rate: 0.25\n",
      "Epoch 0; Step 0; Loss 0.693147; Train acc: 0.500000; Dev acc 0.445312\n",
      "Epoch 5; Step 135; Loss 0.636775; Train acc: 0.753906; Dev acc 0.691406\n",
      "Epoch 10; Step 270; Loss 0.464161; Train acc: 0.859375; Dev acc 0.785156\n",
      "Epoch 15; Step 405; Loss 0.402564; Train acc: 0.945312; Dev acc 0.785156\n",
      "Epoch 20; Step 540; Loss 0.374720; Train acc: 0.980469; Dev acc 0.792969\n",
      "Epoch 25; Step 675; Loss 0.357042; Train acc: 0.984375; Dev acc 0.796875\n",
      "Epoch 30; Step 810; Loss 0.354505; Train acc: 0.992188; Dev acc 0.781250\n",
      "Best dev accuracy is 0.796875\n",
      "\n",
      "(Search 7)\n",
      "    learning_rate: 0.0008\n",
      "    num_layers: 2\n",
      "    drop_rate: 0.35\n",
      "Epoch 0; Step 0; Loss 0.693146; Train acc: 0.492188; Dev acc 0.511719\n",
      "Epoch 5; Step 135; Loss 0.626356; Train acc: 0.726562; Dev acc 0.730469\n",
      "Epoch 10; Step 270; Loss 0.435620; Train acc: 0.898438; Dev acc 0.843750\n",
      "Epoch 15; Step 405; Loss 0.421537; Train acc: 0.960938; Dev acc 0.847656\n",
      "Epoch 20; Step 540; Loss 0.384115; Train acc: 0.976562; Dev acc 0.824219\n",
      "Epoch 25; Step 675; Loss 0.392655; Train acc: 0.980469; Dev acc 0.832031\n",
      "Epoch 30; Step 810; Loss 0.350856; Train acc: 0.988281; Dev acc 0.824219\n",
      "Best dev accuracy is 0.84765625\n",
      "\n",
      "(Search 8)\n",
      "    learning_rate: 0.0011\n",
      "    num_layers: 1\n",
      "    drop_rate: 0.35\n",
      "Epoch 0; Step 0; Loss 0.693131; Train acc: 0.519531; Dev acc 0.531250\n",
      "Epoch 5; Step 135; Loss 0.650054; Train acc: 0.718750; Dev acc 0.699219\n",
      "Epoch 10; Step 270; Loss 0.473485; Train acc: 0.878906; Dev acc 0.796875\n",
      "Epoch 15; Step 405; Loss 0.422313; Train acc: 0.941406; Dev acc 0.789062\n",
      "Epoch 20; Step 540; Loss 0.409909; Train acc: 0.964844; Dev acc 0.789062\n",
      "Epoch 25; Step 675; Loss 0.373622; Train acc: 0.972656; Dev acc 0.773438\n",
      "Epoch 30; Step 810; Loss 0.361488; Train acc: 0.976562; Dev acc 0.769531\n",
      "Best dev accuracy is 0.796875\n",
      "\n",
      "(Search 9)\n",
      "    learning_rate: 0.0010\n",
      "    num_layers: 1\n",
      "    drop_rate: 0.35\n",
      "Epoch 0; Step 0; Loss 0.693194; Train acc: 0.535156; Dev acc 0.492188\n",
      "Epoch 5; Step 135; Loss 0.659867; Train acc: 0.585938; Dev acc 0.609375\n",
      "Epoch 10; Step 270; Loss 0.518726; Train acc: 0.828125; Dev acc 0.730469\n",
      "Epoch 15; Step 405; Loss 0.436688; Train acc: 0.917969; Dev acc 0.804688\n",
      "Epoch 20; Step 540; Loss 0.409794; Train acc: 0.953125; Dev acc 0.820312\n",
      "Epoch 25; Step 675; Loss 0.400160; Train acc: 0.972656; Dev acc 0.820312\n",
      "Epoch 30; Step 810; Loss 0.380115; Train acc: 0.980469; Dev acc 0.824219\n",
      "Best dev accuracy is 0.82421875\n",
      "\n",
      "(Search 10)\n",
      "    learning_rate: 0.0012\n",
      "    num_layers: 4\n",
      "    drop_rate: 0.35\n",
      "Epoch 0; Step 0; Loss 0.693147; Train acc: 0.464844; Dev acc 0.507812\n",
      "Epoch 5; Step 135; Loss 0.469694; Train acc: 0.847656; Dev acc 0.789062\n",
      "Epoch 10; Step 270; Loss 0.406003; Train acc: 0.945312; Dev acc 0.796875\n",
      "Epoch 15; Step 405; Loss 0.385346; Train acc: 0.964844; Dev acc 0.796875\n",
      "Epoch 20; Step 540; Loss 0.382820; Train acc: 0.972656; Dev acc 0.812500\n",
      "Epoch 25; Step 675; Loss 0.361904; Train acc: 0.980469; Dev acc 0.812500\n",
      "Epoch 30; Step 810; Loss 0.374444; Train acc: 0.988281; Dev acc 0.792969\n",
      "Best dev accuracy is 0.8125\n",
      "\n",
      "(Search 11)\n",
      "    learning_rate: 0.0010\n",
      "    num_layers: 4\n",
      "    drop_rate: 0.30\n",
      "Epoch 0; Step 0; Loss 0.693147; Train acc: 0.457031; Dev acc 0.457031\n",
      "Epoch 5; Step 135; Loss 0.488537; Train acc: 0.855469; Dev acc 0.726562\n",
      "Epoch 10; Step 270; Loss 0.379884; Train acc: 0.968750; Dev acc 0.761719\n",
      "Epoch 15; Step 405; Loss 0.374763; Train acc: 0.988281; Dev acc 0.742188\n",
      "Epoch 20; Step 540; Loss 0.380816; Train acc: 0.972656; Dev acc 0.757812\n",
      "Epoch 25; Step 675; Loss 0.374325; Train acc: 0.988281; Dev acc 0.738281\n",
      "Epoch 30; Step 810; Loss 0.375326; Train acc: 0.992188; Dev acc 0.714844\n",
      "Best dev accuracy is 0.76171875\n",
      "\n",
      "(Search 12)\n",
      "    learning_rate: 0.0010\n",
      "    num_layers: 2\n",
      "    drop_rate: 0.25\n",
      "Epoch 0; Step 0; Loss 0.693151; Train acc: 0.492188; Dev acc 0.511719\n",
      "Epoch 5; Step 135; Loss 0.575825; Train acc: 0.777344; Dev acc 0.707031\n",
      "Epoch 10; Step 270; Loss 0.391482; Train acc: 0.917969; Dev acc 0.765625\n",
      "Epoch 15; Step 405; Loss 0.367505; Train acc: 0.972656; Dev acc 0.750000\n",
      "Epoch 20; Step 540; Loss 0.366207; Train acc: 0.984375; Dev acc 0.734375\n",
      "Epoch 25; Step 675; Loss 0.342562; Train acc: 0.996094; Dev acc 0.742188\n",
      "Epoch 30; Step 810; Loss 0.336859; Train acc: 0.992188; Dev acc 0.714844\n",
      "Best dev accuracy is 0.765625\n",
      "\n",
      "(Search 13)\n",
      "    learning_rate: 0.0012\n",
      "    num_layers: 4\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693147; Train acc: 0.511719; Dev acc 0.429688\n",
      "Epoch 5; Step 135; Loss 0.467298; Train acc: 0.851562; Dev acc 0.781250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10; Step 270; Loss 0.369361; Train acc: 0.949219; Dev acc 0.792969\n",
      "Epoch 15; Step 405; Loss 0.350226; Train acc: 0.964844; Dev acc 0.781250\n",
      "Epoch 20; Step 540; Loss 0.356623; Train acc: 0.972656; Dev acc 0.800781\n",
      "Epoch 25; Step 675; Loss 0.333981; Train acc: 0.980469; Dev acc 0.789062\n",
      "Epoch 30; Step 810; Loss 0.331113; Train acc: 0.980469; Dev acc 0.804688\n",
      "Best dev accuracy is 0.8046875\n",
      "\n",
      "(Search 14)\n",
      "    learning_rate: 0.0009\n",
      "    num_layers: 4\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693146; Train acc: 0.503906; Dev acc 0.519531\n",
      "Epoch 5; Step 135; Loss 0.543360; Train acc: 0.785156; Dev acc 0.738281\n",
      "Epoch 10; Step 270; Loss 0.385168; Train acc: 0.957031; Dev acc 0.789062\n",
      "Epoch 15; Step 405; Loss 0.381304; Train acc: 0.976562; Dev acc 0.781250\n",
      "Epoch 20; Step 540; Loss 0.365933; Train acc: 0.976562; Dev acc 0.773438\n",
      "Epoch 25; Step 675; Loss 0.356542; Train acc: 0.980469; Dev acc 0.769531\n",
      "Epoch 30; Step 810; Loss 0.343049; Train acc: 0.980469; Dev acc 0.765625\n",
      "Best dev accuracy is 0.7890625\n",
      "\n",
      "(Search 15)\n",
      "    learning_rate: 0.0012\n",
      "    num_layers: 2\n",
      "    drop_rate: 0.35\n",
      "Epoch 0; Step 0; Loss 0.693155; Train acc: 0.523438; Dev acc 0.566406\n",
      "Epoch 5; Step 135; Loss 0.509541; Train acc: 0.800781; Dev acc 0.730469\n",
      "Epoch 10; Step 270; Loss 0.403988; Train acc: 0.953125; Dev acc 0.796875\n",
      "Epoch 15; Step 405; Loss 0.388171; Train acc: 0.976562; Dev acc 0.785156\n",
      "Epoch 20; Step 540; Loss 0.362087; Train acc: 0.984375; Dev acc 0.789062\n",
      "Epoch 25; Step 675; Loss 0.363256; Train acc: 0.996094; Dev acc 0.781250\n",
      "Epoch 30; Step 810; Loss 0.370585; Train acc: 0.992188; Dev acc 0.781250\n",
      "Best dev accuracy is 0.796875\n",
      "\n",
      "(Search 16)\n",
      "    learning_rate: 0.0009\n",
      "    num_layers: 4\n",
      "    drop_rate: 0.30\n",
      "Epoch 0; Step 0; Loss 0.693147; Train acc: 0.515625; Dev acc 0.531250\n",
      "Epoch 5; Step 135; Loss 0.528265; Train acc: 0.773438; Dev acc 0.750000\n",
      "Epoch 10; Step 270; Loss 0.403306; Train acc: 0.925781; Dev acc 0.808594\n",
      "Epoch 15; Step 405; Loss 0.361461; Train acc: 0.949219; Dev acc 0.800781\n",
      "Epoch 20; Step 540; Loss 0.373013; Train acc: 0.960938; Dev acc 0.796875\n",
      "Epoch 25; Step 675; Loss 0.347988; Train acc: 0.972656; Dev acc 0.796875\n",
      "Epoch 30; Step 810; Loss 0.355002; Train acc: 0.972656; Dev acc 0.769531\n",
      "Best dev accuracy is 0.80859375\n",
      "\n",
      "(Search 17)\n",
      "    learning_rate: 0.0010\n",
      "    num_layers: 3\n",
      "    drop_rate: 0.25\n",
      "Epoch 0; Step 0; Loss 0.693150; Train acc: 0.519531; Dev acc 0.539062\n",
      "Epoch 5; Step 135; Loss 0.523185; Train acc: 0.816406; Dev acc 0.683594\n",
      "Epoch 10; Step 270; Loss 0.386178; Train acc: 0.976562; Dev acc 0.781250\n",
      "Epoch 15; Step 405; Loss 0.380341; Train acc: 0.972656; Dev acc 0.789062\n",
      "Epoch 20; Step 540; Loss 0.358492; Train acc: 0.984375; Dev acc 0.777344\n",
      "Epoch 25; Step 675; Loss 0.361543; Train acc: 0.996094; Dev acc 0.773438\n",
      "Epoch 30; Step 810; Loss 0.368159; Train acc: 1.000000; Dev acc 0.750000\n",
      "Best dev accuracy is 0.7890625\n",
      "\n",
      "(Search 18)\n",
      "    learning_rate: 0.0011\n",
      "    num_layers: 1\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693073; Train acc: 0.507812; Dev acc 0.507812\n",
      "Epoch 5; Step 135; Loss 0.640378; Train acc: 0.734375; Dev acc 0.679688\n",
      "Epoch 10; Step 270; Loss 0.463679; Train acc: 0.863281; Dev acc 0.746094\n",
      "Epoch 15; Step 405; Loss 0.395632; Train acc: 0.949219; Dev acc 0.746094\n",
      "Epoch 20; Step 540; Loss 0.380891; Train acc: 0.976562; Dev acc 0.746094\n",
      "Epoch 25; Step 675; Loss 0.358972; Train acc: 0.988281; Dev acc 0.742188\n",
      "Epoch 30; Step 810; Loss 0.355362; Train acc: 0.988281; Dev acc 0.742188\n",
      "Best dev accuracy is 0.74609375\n",
      "\n",
      "(Search 19)\n",
      "    learning_rate: 0.0009\n",
      "    num_layers: 4\n",
      "    drop_rate: 0.35\n",
      "Epoch 0; Step 0; Loss 0.693148; Train acc: 0.531250; Dev acc 0.566406\n",
      "Epoch 5; Step 135; Loss 0.508728; Train acc: 0.777344; Dev acc 0.757812\n",
      "Epoch 10; Step 270; Loss 0.391183; Train acc: 0.921875; Dev acc 0.816406\n",
      "Epoch 15; Step 405; Loss 0.404913; Train acc: 0.960938; Dev acc 0.792969\n",
      "Epoch 20; Step 540; Loss 0.376651; Train acc: 0.960938; Dev acc 0.773438\n",
      "Epoch 25; Step 675; Loss 0.390178; Train acc: 0.972656; Dev acc 0.773438\n",
      "Epoch 30; Step 810; Loss 0.375385; Train acc: 0.972656; Dev acc 0.769531\n",
      "Best dev accuracy is 0.81640625\n",
      "\n",
      "(Search 20)\n",
      "    learning_rate: 0.0008\n",
      "    num_layers: 3\n",
      "    drop_rate: 0.25\n",
      "Epoch 0; Step 0; Loss 0.693147; Train acc: 0.539062; Dev acc 0.515625\n",
      "Epoch 5; Step 135; Loss 0.573823; Train acc: 0.753906; Dev acc 0.687500\n",
      "Epoch 10; Step 270; Loss 0.400344; Train acc: 0.957031; Dev acc 0.781250\n",
      "Epoch 15; Step 405; Loss 0.369614; Train acc: 0.980469; Dev acc 0.757812\n",
      "Epoch 20; Step 540; Loss 0.359909; Train acc: 0.980469; Dev acc 0.746094\n",
      "Epoch 25; Step 675; Loss 0.352365; Train acc: 0.988281; Dev acc 0.722656\n",
      "Epoch 30; Step 810; Loss 0.367915; Train acc: 0.988281; Dev acc 0.746094\n",
      "Best dev accuracy is 0.78125\n",
      "\n",
      "(Search 21)\n",
      "    learning_rate: 0.0012\n",
      "    num_layers: 1\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693150; Train acc: 0.500000; Dev acc 0.464844\n",
      "Epoch 5; Step 135; Loss 0.619377; Train acc: 0.796875; Dev acc 0.726562\n",
      "Epoch 10; Step 270; Loss 0.466202; Train acc: 0.910156; Dev acc 0.808594\n",
      "Epoch 15; Step 405; Loss 0.385785; Train acc: 0.964844; Dev acc 0.812500\n",
      "Epoch 20; Step 540; Loss 0.367608; Train acc: 0.980469; Dev acc 0.808594\n",
      "Epoch 25; Step 675; Loss 0.358948; Train acc: 0.988281; Dev acc 0.816406\n",
      "Epoch 30; Step 810; Loss 0.351560; Train acc: 0.988281; Dev acc 0.816406\n",
      "Best dev accuracy is 0.81640625\n",
      "\n",
      "(Search 22)\n",
      "    learning_rate: 0.0009\n",
      "    num_layers: 3\n",
      "    drop_rate: 0.35\n",
      "Epoch 0; Step 0; Loss 0.693148; Train acc: 0.578125; Dev acc 0.515625\n",
      "Epoch 5; Step 135; Loss 0.552657; Train acc: 0.769531; Dev acc 0.707031\n",
      "Epoch 10; Step 270; Loss 0.404365; Train acc: 0.949219; Dev acc 0.781250\n",
      "Epoch 15; Step 405; Loss 0.376005; Train acc: 0.976562; Dev acc 0.777344\n",
      "Epoch 20; Step 540; Loss 0.386584; Train acc: 0.976562; Dev acc 0.757812\n",
      "Epoch 25; Step 675; Loss 0.375789; Train acc: 0.992188; Dev acc 0.750000\n",
      "Epoch 30; Step 810; Loss 0.358853; Train acc: 0.988281; Dev acc 0.769531\n",
      "Best dev accuracy is 0.78125\n",
      "\n",
      "(Search 23)\n",
      "    learning_rate: 0.0010\n",
      "    num_layers: 1\n",
      "    drop_rate: 0.35\n",
      "Epoch 0; Step 0; Loss 0.693139; Train acc: 0.523438; Dev acc 0.589844\n",
      "Epoch 5; Step 135; Loss 0.660021; Train acc: 0.691406; Dev acc 0.695312\n",
      "Epoch 10; Step 270; Loss 0.519606; Train acc: 0.886719; Dev acc 0.789062\n",
      "Epoch 15; Step 405; Loss 0.443672; Train acc: 0.945312; Dev acc 0.839844\n",
      "Epoch 20; Step 540; Loss 0.409786; Train acc: 0.972656; Dev acc 0.835938\n",
      "Epoch 25; Step 675; Loss 0.417446; Train acc: 0.980469; Dev acc 0.824219\n",
      "Epoch 30; Step 810; Loss 0.370064; Train acc: 0.988281; Dev acc 0.824219\n",
      "Best dev accuracy is 0.83984375\n",
      "\n",
      "(Search 24)\n",
      "    learning_rate: 0.0011\n",
      "    num_layers: 1\n",
      "    drop_rate: 0.30\n",
      "Epoch 0; Step 0; Loss 0.693141; Train acc: 0.527344; Dev acc 0.523438\n",
      "Epoch 5; Step 135; Loss 0.647674; Train acc: 0.765625; Dev acc 0.683594\n",
      "Epoch 10; Step 270; Loss 0.489472; Train acc: 0.890625; Dev acc 0.785156\n",
      "Epoch 15; Step 405; Loss 0.433082; Train acc: 0.953125; Dev acc 0.800781\n",
      "Epoch 20; Step 540; Loss 0.407422; Train acc: 0.964844; Dev acc 0.789062\n",
      "Epoch 25; Step 675; Loss 0.365636; Train acc: 0.976562; Dev acc 0.781250\n",
      "Epoch 30; Step 810; Loss 0.365556; Train acc: 0.984375; Dev acc 0.796875\n",
      "Best dev accuracy is 0.80078125\n",
      "\n",
      "(Search 25)\n",
      "    learning_rate: 0.0011\n",
      "    num_layers: 3\n",
      "    drop_rate: 0.35\n",
      "Epoch 0; Step 0; Loss 0.693148; Train acc: 0.503906; Dev acc 0.539062\n",
      "Epoch 5; Step 135; Loss 0.485978; Train acc: 0.847656; Dev acc 0.750000\n",
      "Epoch 10; Step 270; Loss 0.402854; Train acc: 0.964844; Dev acc 0.824219\n",
      "Epoch 15; Step 405; Loss 0.411954; Train acc: 0.976562; Dev acc 0.828125\n",
      "Epoch 20; Step 540; Loss 0.414741; Train acc: 0.976562; Dev acc 0.812500\n",
      "Epoch 25; Step 675; Loss 0.370968; Train acc: 0.984375; Dev acc 0.781250\n",
      "Epoch 30; Step 810; Loss 0.364227; Train acc: 0.988281; Dev acc 0.816406\n",
      "Best dev accuracy is 0.828125\n",
      "\n",
      "(Search 26)\n",
      "    learning_rate: 0.0010\n",
      "    num_layers: 3\n",
      "    drop_rate: 0.30\n",
      "Epoch 0; Step 0; Loss 0.693148; Train acc: 0.503906; Dev acc 0.539062\n",
      "Epoch 5; Step 135; Loss 0.534518; Train acc: 0.800781; Dev acc 0.769531\n",
      "Epoch 10; Step 270; Loss 0.401280; Train acc: 0.953125; Dev acc 0.785156\n",
      "Epoch 15; Step 405; Loss 0.377688; Train acc: 0.960938; Dev acc 0.773438\n",
      "Epoch 20; Step 540; Loss 0.358937; Train acc: 0.980469; Dev acc 0.765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25; Step 675; Loss 0.372909; Train acc: 0.980469; Dev acc 0.753906\n",
      "Epoch 30; Step 810; Loss 0.359211; Train acc: 0.984375; Dev acc 0.773438\n",
      "Best dev accuracy is 0.78515625\n",
      "\n",
      "(Search 27)\n",
      "    learning_rate: 0.0009\n",
      "    num_layers: 1\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693102; Train acc: 0.527344; Dev acc 0.562500\n",
      "Epoch 5; Step 135; Loss 0.657282; Train acc: 0.656250; Dev acc 0.617188\n",
      "Epoch 10; Step 270; Loss 0.486302; Train acc: 0.875000; Dev acc 0.769531\n",
      "Epoch 15; Step 405; Loss 0.445535; Train acc: 0.937500; Dev acc 0.781250\n",
      "Epoch 20; Step 540; Loss 0.407196; Train acc: 0.972656; Dev acc 0.777344\n",
      "Epoch 25; Step 675; Loss 0.369116; Train acc: 0.984375; Dev acc 0.781250\n",
      "Epoch 30; Step 810; Loss 0.364356; Train acc: 0.988281; Dev acc 0.777344\n",
      "Best dev accuracy is 0.78125\n",
      "\n",
      "(Search 28)\n",
      "    learning_rate: 0.0011\n",
      "    num_layers: 1\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693136; Train acc: 0.523438; Dev acc 0.531250\n",
      "Epoch 5; Step 135; Loss 0.631185; Train acc: 0.800781; Dev acc 0.699219\n",
      "Epoch 10; Step 270; Loss 0.493805; Train acc: 0.902344; Dev acc 0.769531\n",
      "Epoch 15; Step 405; Loss 0.397146; Train acc: 0.964844; Dev acc 0.781250\n",
      "Epoch 20; Step 540; Loss 0.369745; Train acc: 0.984375; Dev acc 0.789062\n",
      "Epoch 25; Step 675; Loss 0.365326; Train acc: 0.992188; Dev acc 0.777344\n",
      "Epoch 30; Step 810; Loss 0.338086; Train acc: 0.988281; Dev acc 0.773438\n",
      "Best dev accuracy is 0.7890625\n",
      "\n",
      "(Search 29)\n",
      "    learning_rate: 0.0010\n",
      "    num_layers: 3\n",
      "    drop_rate: 0.30\n",
      "Epoch 0; Step 0; Loss 0.693146; Train acc: 0.519531; Dev acc 0.515625\n",
      "Epoch 5; Step 135; Loss 0.511431; Train acc: 0.824219; Dev acc 0.734375\n",
      "Epoch 10; Step 270; Loss 0.389910; Train acc: 0.964844; Dev acc 0.781250\n",
      "Epoch 15; Step 405; Loss 0.358566; Train acc: 0.972656; Dev acc 0.746094\n",
      "Epoch 20; Step 540; Loss 0.361237; Train acc: 0.984375; Dev acc 0.742188\n",
      "Epoch 25; Step 675; Loss 0.357424; Train acc: 0.988281; Dev acc 0.750000\n",
      "Epoch 30; Step 810; Loss 0.355080; Train acc: 0.988281; Dev acc 0.730469\n",
      "Best dev accuracy is 0.78125\n",
      "\n",
      "(Search 30)\n",
      "    learning_rate: 0.0012\n",
      "    num_layers: 1\n",
      "    drop_rate: 0.30\n",
      "Epoch 0; Step 0; Loss 0.693195; Train acc: 0.515625; Dev acc 0.539062\n",
      "Epoch 5; Step 135; Loss 0.641454; Train acc: 0.757812; Dev acc 0.714844\n",
      "Epoch 10; Step 270; Loss 0.470004; Train acc: 0.894531; Dev acc 0.816406\n",
      "Epoch 15; Step 405; Loss 0.407624; Train acc: 0.964844; Dev acc 0.820312\n",
      "Epoch 20; Step 540; Loss 0.380345; Train acc: 0.980469; Dev acc 0.804688\n",
      "Epoch 25; Step 675; Loss 0.376570; Train acc: 0.984375; Dev acc 0.808594\n",
      "Epoch 30; Step 810; Loss 0.358320; Train acc: 0.988281; Dev acc 0.800781\n",
      "Best dev accuracy is 0.8203125\n",
      "\n",
      "(Search 31)\n",
      "    learning_rate: 0.0011\n",
      "    num_layers: 3\n",
      "    drop_rate: 0.35\n",
      "Epoch 0; Step 0; Loss 0.693148; Train acc: 0.527344; Dev acc 0.453125\n",
      "Epoch 5; Step 135; Loss 0.524327; Train acc: 0.761719; Dev acc 0.675781\n",
      "Epoch 10; Step 270; Loss 0.417911; Train acc: 0.941406; Dev acc 0.777344\n",
      "Epoch 15; Step 405; Loss 0.398896; Train acc: 0.964844; Dev acc 0.812500\n",
      "Epoch 20; Step 540; Loss 0.366338; Train acc: 0.976562; Dev acc 0.796875\n",
      "Epoch 25; Step 675; Loss 0.366442; Train acc: 0.976562; Dev acc 0.796875\n",
      "Epoch 30; Step 810; Loss 0.349975; Train acc: 0.988281; Dev acc 0.789062\n",
      "Best dev accuracy is 0.8125\n",
      "\n",
      "(Search 32)\n",
      "    learning_rate: 0.0011\n",
      "    num_layers: 4\n",
      "    drop_rate: 0.30\n",
      "Epoch 0; Step 0; Loss 0.693146; Train acc: 0.464844; Dev acc 0.542969\n",
      "Epoch 5; Step 135; Loss 0.456636; Train acc: 0.835938; Dev acc 0.761719\n",
      "Epoch 10; Step 270; Loss 0.426232; Train acc: 0.949219; Dev acc 0.777344\n",
      "Epoch 15; Step 405; Loss 0.354362; Train acc: 0.972656; Dev acc 0.761719\n",
      "Epoch 20; Step 540; Loss 0.356869; Train acc: 0.980469; Dev acc 0.785156\n",
      "Epoch 25; Step 675; Loss 0.372110; Train acc: 0.972656; Dev acc 0.753906\n",
      "Epoch 30; Step 810; Loss 0.371013; Train acc: 0.980469; Dev acc 0.753906\n",
      "Best dev accuracy is 0.78515625\n",
      "\n",
      "(Search 33)\n",
      "    learning_rate: 0.0008\n",
      "    num_layers: 2\n",
      "    drop_rate: 0.25\n",
      "Epoch 0; Step 0; Loss 0.693149; Train acc: 0.507812; Dev acc 0.480469\n",
      "Epoch 5; Step 135; Loss 0.625257; Train acc: 0.792969; Dev acc 0.714844\n",
      "Epoch 10; Step 270; Loss 0.410127; Train acc: 0.937500; Dev acc 0.835938\n",
      "Epoch 15; Step 405; Loss 0.377093; Train acc: 0.968750; Dev acc 0.828125\n",
      "Epoch 20; Step 540; Loss 0.363148; Train acc: 0.988281; Dev acc 0.839844\n",
      "Epoch 25; Step 675; Loss 0.345479; Train acc: 0.988281; Dev acc 0.855469\n",
      "Epoch 30; Step 810; Loss 0.351000; Train acc: 0.988281; Dev acc 0.843750\n",
      "Best dev accuracy is 0.85546875\n",
      "\n",
      "(Search 34)\n",
      "    learning_rate: 0.0010\n",
      "    num_layers: 2\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693145; Train acc: 0.535156; Dev acc 0.519531\n",
      "Epoch 5; Step 135; Loss 0.536963; Train acc: 0.769531; Dev acc 0.730469\n",
      "Epoch 10; Step 270; Loss 0.383799; Train acc: 0.957031; Dev acc 0.812500\n",
      "Epoch 15; Step 405; Loss 0.350296; Train acc: 0.984375; Dev acc 0.804688\n",
      "Epoch 20; Step 540; Loss 0.349661; Train acc: 0.988281; Dev acc 0.804688\n",
      "Epoch 25; Step 675; Loss 0.354868; Train acc: 0.992188; Dev acc 0.769531\n",
      "Epoch 30; Step 810; Loss 0.345276; Train acc: 0.992188; Dev acc 0.789062\n",
      "Best dev accuracy is 0.8125\n",
      "\n",
      "(Search 35)\n",
      "    learning_rate: 0.0008\n",
      "    num_layers: 1\n",
      "    drop_rate: 0.25\n",
      "Epoch 0; Step 0; Loss 0.693153; Train acc: 0.484375; Dev acc 0.492188\n",
      "Epoch 5; Step 135; Loss 0.676868; Train acc: 0.550781; Dev acc 0.511719\n",
      "Epoch 10; Step 270; Loss 0.542759; Train acc: 0.832031; Dev acc 0.753906\n",
      "Epoch 15; Step 405; Loss 0.462600; Train acc: 0.917969; Dev acc 0.816406\n",
      "Epoch 20; Step 540; Loss 0.430063; Train acc: 0.957031; Dev acc 0.800781\n",
      "Epoch 25; Step 675; Loss 0.401474; Train acc: 0.980469; Dev acc 0.789062\n",
      "Epoch 30; Step 810; Loss 0.383449; Train acc: 0.984375; Dev acc 0.800781\n",
      "Best dev accuracy is 0.81640625\n",
      "\n",
      "(Search 36)\n",
      "    learning_rate: 0.0012\n",
      "    num_layers: 2\n",
      "    drop_rate: 0.30\n",
      "Epoch 0; Step 0; Loss 0.693148; Train acc: 0.472656; Dev acc 0.460938\n",
      "Epoch 5; Step 135; Loss 0.520502; Train acc: 0.808594; Dev acc 0.742188\n",
      "Epoch 10; Step 270; Loss 0.405944; Train acc: 0.960938; Dev acc 0.816406\n",
      "Epoch 15; Step 405; Loss 0.369800; Train acc: 0.980469; Dev acc 0.839844\n",
      "Epoch 20; Step 540; Loss 0.348322; Train acc: 0.984375; Dev acc 0.812500\n",
      "Epoch 25; Step 675; Loss 0.327788; Train acc: 0.988281; Dev acc 0.808594\n",
      "Epoch 30; Step 810; Loss 0.353714; Train acc: 0.988281; Dev acc 0.800781\n",
      "Best dev accuracy is 0.83984375\n",
      "\n",
      "(Search 37)\n",
      "    learning_rate: 0.0012\n",
      "    num_layers: 4\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693147; Train acc: 0.503906; Dev acc 0.468750\n",
      "Epoch 5; Step 135; Loss 0.448848; Train acc: 0.863281; Dev acc 0.785156\n",
      "Epoch 10; Step 270; Loss 0.369210; Train acc: 0.953125; Dev acc 0.792969\n",
      "Epoch 15; Step 405; Loss 0.379637; Train acc: 0.984375; Dev acc 0.773438\n",
      "Epoch 20; Step 540; Loss 0.354506; Train acc: 0.988281; Dev acc 0.765625\n",
      "Epoch 25; Step 675; Loss 0.352341; Train acc: 0.992188; Dev acc 0.761719\n",
      "Epoch 30; Step 810; Loss 0.334875; Train acc: 0.992188; Dev acc 0.757812\n",
      "Best dev accuracy is 0.79296875\n",
      "\n",
      "(Search 38)\n",
      "    learning_rate: 0.0010\n",
      "    num_layers: 1\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693139; Train acc: 0.511719; Dev acc 0.539062\n",
      "Epoch 5; Step 135; Loss 0.650492; Train acc: 0.683594; Dev acc 0.703125\n",
      "Epoch 10; Step 270; Loss 0.499220; Train acc: 0.855469; Dev acc 0.789062\n",
      "Epoch 15; Step 405; Loss 0.404477; Train acc: 0.933594; Dev acc 0.820312\n",
      "Epoch 20; Step 540; Loss 0.374323; Train acc: 0.953125; Dev acc 0.812500\n",
      "Epoch 25; Step 675; Loss 0.349687; Train acc: 0.972656; Dev acc 0.785156\n",
      "Epoch 30; Step 810; Loss 0.363150; Train acc: 0.984375; Dev acc 0.796875\n",
      "Best dev accuracy is 0.8203125\n",
      "\n",
      "(Search 39)\n",
      "    learning_rate: 0.0009\n",
      "    num_layers: 4\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693148; Train acc: 0.511719; Dev acc 0.531250\n",
      "Epoch 5; Step 135; Loss 0.530621; Train acc: 0.835938; Dev acc 0.773438\n",
      "Epoch 10; Step 270; Loss 0.368642; Train acc: 0.972656; Dev acc 0.808594\n",
      "Epoch 15; Step 405; Loss 0.357319; Train acc: 0.972656; Dev acc 0.800781\n",
      "Epoch 20; Step 540; Loss 0.346213; Train acc: 0.984375; Dev acc 0.757812\n",
      "Epoch 25; Step 675; Loss 0.351997; Train acc: 0.988281; Dev acc 0.789062\n",
      "Epoch 30; Step 810; Loss 0.343189; Train acc: 0.992188; Dev acc 0.773438\n",
      "Best dev accuracy is 0.80859375\n",
      "\n",
      "(Search 40)\n",
      "    learning_rate: 0.0008\n",
      "    num_layers: 4\n",
      "    drop_rate: 0.25\n",
      "Epoch 0; Step 0; Loss 0.693148; Train acc: 0.488281; Dev acc 0.484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5; Step 135; Loss 0.589260; Train acc: 0.742188; Dev acc 0.691406\n",
      "Epoch 10; Step 270; Loss 0.384187; Train acc: 0.957031; Dev acc 0.792969\n",
      "Epoch 15; Step 405; Loss 0.371384; Train acc: 0.972656; Dev acc 0.785156\n",
      "Epoch 20; Step 540; Loss 0.365870; Train acc: 0.980469; Dev acc 0.789062\n",
      "Epoch 25; Step 675; Loss 0.362673; Train acc: 0.992188; Dev acc 0.781250\n",
      "Epoch 30; Step 810; Loss 0.342476; Train acc: 0.992188; Dev acc 0.785156\n",
      "Best dev accuracy is 0.79296875\n",
      "\n",
      "(Search 41)\n",
      "    learning_rate: 0.0012\n",
      "    num_layers: 4\n",
      "    drop_rate: 0.25\n",
      "Epoch 0; Step 0; Loss 0.693147; Train acc: 0.492188; Dev acc 0.460938\n",
      "Epoch 5; Step 135; Loss 0.481780; Train acc: 0.855469; Dev acc 0.738281\n",
      "Epoch 10; Step 270; Loss 0.367183; Train acc: 0.968750; Dev acc 0.765625\n",
      "Epoch 15; Step 405; Loss 0.349188; Train acc: 0.984375; Dev acc 0.757812\n",
      "Epoch 20; Step 540; Loss 0.338788; Train acc: 0.980469; Dev acc 0.761719\n",
      "Epoch 25; Step 675; Loss 0.366116; Train acc: 0.996094; Dev acc 0.765625\n",
      "Epoch 30; Step 810; Loss 0.338011; Train acc: 0.996094; Dev acc 0.742188\n",
      "Best dev accuracy is 0.765625\n",
      "\n",
      "(Search 42)\n",
      "    learning_rate: 0.0010\n",
      "    num_layers: 2\n",
      "    drop_rate: 0.30\n",
      "Epoch 0; Step 0; Loss 0.693149; Train acc: 0.519531; Dev acc 0.519531\n",
      "Epoch 5; Step 135; Loss 0.579431; Train acc: 0.777344; Dev acc 0.691406\n",
      "Epoch 10; Step 270; Loss 0.415793; Train acc: 0.937500; Dev acc 0.777344\n",
      "Epoch 15; Step 405; Loss 0.369894; Train acc: 0.968750; Dev acc 0.792969\n",
      "Epoch 20; Step 540; Loss 0.370025; Train acc: 0.984375; Dev acc 0.773438\n",
      "Epoch 25; Step 675; Loss 0.355107; Train acc: 0.996094; Dev acc 0.757812\n",
      "Epoch 30; Step 810; Loss 0.354040; Train acc: 0.996094; Dev acc 0.761719\n",
      "Best dev accuracy is 0.79296875\n",
      "\n",
      "(Search 43)\n",
      "    learning_rate: 0.0008\n",
      "    num_layers: 3\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693150; Train acc: 0.496094; Dev acc 0.542969\n",
      "Epoch 5; Step 135; Loss 0.573531; Train acc: 0.800781; Dev acc 0.679688\n",
      "Epoch 10; Step 270; Loss 0.390693; Train acc: 0.937500; Dev acc 0.816406\n",
      "Epoch 15; Step 405; Loss 0.356081; Train acc: 0.968750; Dev acc 0.800781\n",
      "Epoch 20; Step 540; Loss 0.371927; Train acc: 0.988281; Dev acc 0.792969\n",
      "Epoch 25; Step 675; Loss 0.345697; Train acc: 0.992188; Dev acc 0.804688\n",
      "Epoch 30; Step 810; Loss 0.344060; Train acc: 0.992188; Dev acc 0.796875\n",
      "Best dev accuracy is 0.81640625\n",
      "\n",
      "(Search 44)\n",
      "    learning_rate: 0.0008\n",
      "    num_layers: 3\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693141; Train acc: 0.539062; Dev acc 0.507812\n",
      "Epoch 5; Step 135; Loss 0.593215; Train acc: 0.761719; Dev acc 0.703125\n",
      "Epoch 10; Step 270; Loss 0.428486; Train acc: 0.941406; Dev acc 0.757812\n",
      "Epoch 15; Step 405; Loss 0.356402; Train acc: 0.984375; Dev acc 0.757812\n",
      "Epoch 20; Step 540; Loss 0.331061; Train acc: 0.988281; Dev acc 0.753906\n",
      "Epoch 25; Step 675; Loss 0.347033; Train acc: 0.996094; Dev acc 0.753906\n",
      "Epoch 30; Step 810; Loss 0.328131; Train acc: 0.992188; Dev acc 0.757812\n",
      "Best dev accuracy is 0.7578125\n",
      "\n",
      "(Search 45)\n",
      "    learning_rate: 0.0011\n",
      "    num_layers: 4\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693148; Train acc: 0.519531; Dev acc 0.535156\n",
      "Epoch 5; Step 135; Loss 0.474007; Train acc: 0.878906; Dev acc 0.800781\n",
      "Epoch 10; Step 270; Loss 0.358023; Train acc: 0.972656; Dev acc 0.796875\n",
      "Epoch 15; Step 405; Loss 0.369480; Train acc: 0.980469; Dev acc 0.800781\n",
      "Epoch 20; Step 540; Loss 0.346249; Train acc: 0.988281; Dev acc 0.812500\n",
      "Epoch 25; Step 675; Loss 0.357200; Train acc: 0.984375; Dev acc 0.808594\n",
      "Epoch 30; Step 810; Loss 0.349436; Train acc: 0.984375; Dev acc 0.789062\n",
      "Best dev accuracy is 0.8125\n",
      "\n",
      "(Search 46)\n",
      "    learning_rate: 0.0008\n",
      "    num_layers: 2\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693159; Train acc: 0.527344; Dev acc 0.597656\n",
      "Epoch 5; Step 135; Loss 0.625008; Train acc: 0.699219; Dev acc 0.714844\n",
      "Epoch 10; Step 270; Loss 0.439940; Train acc: 0.890625; Dev acc 0.785156\n",
      "Epoch 15; Step 405; Loss 0.354741; Train acc: 0.953125; Dev acc 0.769531\n",
      "Epoch 20; Step 540; Loss 0.345964; Train acc: 0.984375; Dev acc 0.761719\n",
      "Epoch 25; Step 675; Loss 0.334491; Train acc: 0.984375; Dev acc 0.753906\n",
      "Epoch 30; Step 810; Loss 0.338613; Train acc: 0.988281; Dev acc 0.769531\n",
      "Best dev accuracy is 0.78515625\n",
      "\n",
      "(Search 47)\n",
      "    learning_rate: 0.0012\n",
      "    num_layers: 2\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693144; Train acc: 0.492188; Dev acc 0.496094\n",
      "Epoch 5; Step 135; Loss 0.524374; Train acc: 0.839844; Dev acc 0.765625\n",
      "Epoch 10; Step 270; Loss 0.378382; Train acc: 0.957031; Dev acc 0.816406\n",
      "Epoch 15; Step 405; Loss 0.361421; Train acc: 0.984375; Dev acc 0.808594\n",
      "Epoch 20; Step 540; Loss 0.346995; Train acc: 0.984375; Dev acc 0.796875\n",
      "Epoch 25; Step 675; Loss 0.343408; Train acc: 0.992188; Dev acc 0.812500\n",
      "Epoch 30; Step 810; Loss 0.345048; Train acc: 0.992188; Dev acc 0.804688\n",
      "Best dev accuracy is 0.81640625\n",
      "\n",
      "(Search 48)\n",
      "    learning_rate: 0.0011\n",
      "    num_layers: 1\n",
      "    drop_rate: 0.25\n",
      "Epoch 0; Step 0; Loss 0.693100; Train acc: 0.554688; Dev acc 0.542969\n",
      "Epoch 5; Step 135; Loss 0.640284; Train acc: 0.769531; Dev acc 0.671875\n",
      "Epoch 10; Step 270; Loss 0.487981; Train acc: 0.859375; Dev acc 0.792969\n",
      "Epoch 15; Step 405; Loss 0.423291; Train acc: 0.953125; Dev acc 0.796875\n",
      "Epoch 20; Step 540; Loss 0.382019; Train acc: 0.980469; Dev acc 0.800781\n",
      "Epoch 25; Step 675; Loss 0.374901; Train acc: 0.980469; Dev acc 0.808594\n",
      "Epoch 30; Step 810; Loss 0.352613; Train acc: 0.984375; Dev acc 0.800781\n",
      "Best dev accuracy is 0.80859375\n",
      "\n",
      "(Search 49)\n",
      "    learning_rate: 0.0008\n",
      "    num_layers: 1\n",
      "    drop_rate: 0.25\n",
      "Epoch 0; Step 0; Loss 0.693141; Train acc: 0.515625; Dev acc 0.496094\n",
      "Epoch 5; Step 135; Loss 0.665874; Train acc: 0.546875; Dev acc 0.550781\n",
      "Epoch 10; Step 270; Loss 0.527906; Train acc: 0.851562; Dev acc 0.796875\n",
      "Epoch 15; Step 405; Loss 0.440249; Train acc: 0.906250; Dev acc 0.816406\n",
      "Epoch 20; Step 540; Loss 0.399440; Train acc: 0.953125; Dev acc 0.820312\n",
      "Epoch 25; Step 675; Loss 0.395098; Train acc: 0.976562; Dev acc 0.816406\n",
      "Epoch 30; Step 810; Loss 0.380502; Train acc: 0.984375; Dev acc 0.804688\n",
      "Best dev accuracy is 0.8203125\n",
      "\n",
      "(Search 50)\n",
      "    learning_rate: 0.0011\n",
      "    num_layers: 2\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693148; Train acc: 0.515625; Dev acc 0.566406\n",
      "Epoch 5; Step 135; Loss 0.562132; Train acc: 0.792969; Dev acc 0.738281\n",
      "Epoch 10; Step 270; Loss 0.386474; Train acc: 0.957031; Dev acc 0.816406\n",
      "Epoch 15; Step 405; Loss 0.354262; Train acc: 0.988281; Dev acc 0.824219\n",
      "Epoch 20; Step 540; Loss 0.343732; Train acc: 0.988281; Dev acc 0.804688\n",
      "Epoch 25; Step 675; Loss 0.338754; Train acc: 0.988281; Dev acc 0.808594\n",
      "Epoch 30; Step 810; Loss 0.347066; Train acc: 0.992188; Dev acc 0.804688\n",
      "Best dev accuracy is 0.82421875\n",
      "\n",
      "(Search 51)\n",
      "    learning_rate: 0.0009\n",
      "    num_layers: 1\n",
      "    drop_rate: 0.35\n",
      "Epoch 0; Step 0; Loss 0.693139; Train acc: 0.519531; Dev acc 0.523438\n",
      "Epoch 5; Step 135; Loss 0.663634; Train acc: 0.593750; Dev acc 0.578125\n",
      "Epoch 10; Step 270; Loss 0.525237; Train acc: 0.839844; Dev acc 0.750000\n",
      "Epoch 15; Step 405; Loss 0.457486; Train acc: 0.914062; Dev acc 0.769531\n",
      "Epoch 20; Step 540; Loss 0.419212; Train acc: 0.949219; Dev acc 0.761719\n",
      "Epoch 25; Step 675; Loss 0.388947; Train acc: 0.972656; Dev acc 0.765625\n",
      "Epoch 30; Step 810; Loss 0.382566; Train acc: 0.984375; Dev acc 0.757812\n",
      "Best dev accuracy is 0.76953125\n",
      "\n",
      "(Search 52)\n",
      "    learning_rate: 0.0012\n",
      "    num_layers: 3\n",
      "    drop_rate: 0.25\n",
      "Epoch 0; Step 0; Loss 0.693149; Train acc: 0.535156; Dev acc 0.507812\n",
      "Epoch 5; Step 135; Loss 0.488543; Train acc: 0.878906; Dev acc 0.777344\n",
      "Epoch 10; Step 270; Loss 0.381322; Train acc: 0.964844; Dev acc 0.761719\n",
      "Epoch 15; Step 405; Loss 0.343981; Train acc: 0.976562; Dev acc 0.789062\n",
      "Epoch 20; Step 540; Loss 0.334662; Train acc: 0.980469; Dev acc 0.773438\n",
      "Epoch 25; Step 675; Loss 0.348978; Train acc: 0.992188; Dev acc 0.777344\n",
      "Epoch 30; Step 810; Loss 0.346324; Train acc: 0.984375; Dev acc 0.765625\n",
      "Best dev accuracy is 0.7890625\n",
      "\n",
      "(Search 53)\n",
      "    learning_rate: 0.0009\n",
      "    num_layers: 4\n",
      "    drop_rate: 0.30\n",
      "Epoch 0; Step 0; Loss 0.693148; Train acc: 0.511719; Dev acc 0.558594\n",
      "Epoch 5; Step 135; Loss 0.497390; Train acc: 0.792969; Dev acc 0.730469\n",
      "Epoch 10; Step 270; Loss 0.401902; Train acc: 0.953125; Dev acc 0.765625\n",
      "Epoch 15; Step 405; Loss 0.380130; Train acc: 0.984375; Dev acc 0.750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20; Step 540; Loss 0.389722; Train acc: 0.984375; Dev acc 0.761719\n",
      "Epoch 25; Step 675; Loss 0.346434; Train acc: 0.996094; Dev acc 0.765625\n",
      "Epoch 30; Step 810; Loss 0.373538; Train acc: 0.996094; Dev acc 0.753906\n",
      "Best dev accuracy is 0.765625\n",
      "\n",
      "(Search 54)\n",
      "    learning_rate: 0.0008\n",
      "    num_layers: 3\n",
      "    drop_rate: 0.25\n",
      "Epoch 0; Step 0; Loss 0.693145; Train acc: 0.519531; Dev acc 0.539062\n",
      "Epoch 5; Step 135; Loss 0.552325; Train acc: 0.769531; Dev acc 0.746094\n",
      "Epoch 10; Step 270; Loss 0.395285; Train acc: 0.925781; Dev acc 0.781250\n",
      "Epoch 15; Step 405; Loss 0.358882; Train acc: 0.957031; Dev acc 0.757812\n",
      "Epoch 20; Step 540; Loss 0.347341; Train acc: 0.960938; Dev acc 0.761719\n",
      "Epoch 25; Step 675; Loss 0.341020; Train acc: 0.976562; Dev acc 0.757812\n",
      "Epoch 30; Step 810; Loss 0.347343; Train acc: 0.980469; Dev acc 0.757812\n",
      "Best dev accuracy is 0.78125\n",
      "\n",
      "(Search 55)\n",
      "    learning_rate: 0.0010\n",
      "    num_layers: 3\n",
      "    drop_rate: 0.30\n",
      "Epoch 0; Step 0; Loss 0.693148; Train acc: 0.496094; Dev acc 0.523438\n",
      "Epoch 5; Step 135; Loss 0.551842; Train acc: 0.785156; Dev acc 0.765625\n",
      "Epoch 10; Step 270; Loss 0.381853; Train acc: 0.957031; Dev acc 0.828125\n",
      "Epoch 15; Step 405; Loss 0.394976; Train acc: 0.976562; Dev acc 0.800781\n",
      "Epoch 20; Step 540; Loss 0.388142; Train acc: 0.980469; Dev acc 0.800781\n",
      "Epoch 25; Step 675; Loss 0.370223; Train acc: 0.988281; Dev acc 0.781250\n",
      "Epoch 30; Step 810; Loss 0.356254; Train acc: 0.984375; Dev acc 0.753906\n",
      "Best dev accuracy is 0.828125\n",
      "\n",
      "(Search 56)\n",
      "    learning_rate: 0.0012\n",
      "    num_layers: 1\n",
      "    drop_rate: 0.30\n",
      "Epoch 0; Step 0; Loss 0.693161; Train acc: 0.527344; Dev acc 0.542969\n",
      "Epoch 5; Step 135; Loss 0.628082; Train acc: 0.695312; Dev acc 0.683594\n",
      "Epoch 10; Step 270; Loss 0.477104; Train acc: 0.867188; Dev acc 0.808594\n",
      "Epoch 15; Step 405; Loss 0.410414; Train acc: 0.964844; Dev acc 0.832031\n",
      "Epoch 20; Step 540; Loss 0.396609; Train acc: 0.980469; Dev acc 0.812500\n",
      "Epoch 25; Step 675; Loss 0.345270; Train acc: 0.984375; Dev acc 0.808594\n",
      "Epoch 30; Step 810; Loss 0.376871; Train acc: 0.988281; Dev acc 0.773438\n",
      "Best dev accuracy is 0.83203125\n",
      "\n",
      "(Search 57)\n",
      "    learning_rate: 0.0012\n",
      "    num_layers: 3\n",
      "    drop_rate: 0.20\n",
      "Epoch 0; Step 0; Loss 0.693148; Train acc: 0.535156; Dev acc 0.531250\n",
      "Epoch 5; Step 135; Loss 0.450642; Train acc: 0.878906; Dev acc 0.761719\n",
      "Epoch 10; Step 270; Loss 0.355026; Train acc: 0.968750; Dev acc 0.796875\n",
      "Epoch 15; Step 405; Loss 0.352457; Train acc: 0.984375; Dev acc 0.781250\n",
      "Epoch 20; Step 540; Loss 0.340856; Train acc: 0.984375; Dev acc 0.796875\n",
      "Epoch 25; Step 675; Loss 0.339003; Train acc: 0.988281; Dev acc 0.789062\n",
      "Epoch 30; Step 810; Loss 0.345184; Train acc: 0.996094; Dev acc 0.781250\n",
      "Best dev accuracy is 0.796875\n",
      "\n",
      "(Search 58)\n",
      "    learning_rate: 0.0012\n",
      "    num_layers: 3\n",
      "    drop_rate: 0.25\n",
      "Epoch 0; Step 0; Loss 0.693150; Train acc: 0.542969; Dev acc 0.542969\n",
      "Epoch 5; Step 135; Loss 0.504777; Train acc: 0.820312; Dev acc 0.750000\n",
      "Epoch 10; Step 270; Loss 0.379353; Train acc: 0.980469; Dev acc 0.792969\n",
      "Epoch 15; Step 405; Loss 0.382376; Train acc: 0.988281; Dev acc 0.785156\n",
      "Epoch 20; Step 540; Loss 0.349351; Train acc: 0.992188; Dev acc 0.773438\n",
      "Epoch 25; Step 675; Loss 0.346615; Train acc: 0.996094; Dev acc 0.765625\n",
      "Epoch 30; Step 810; Loss 0.349353; Train acc: 0.984375; Dev acc 0.765625\n",
      "Best dev accuracy is 0.79296875\n",
      "\n",
      "(Search 59)\n",
      "    learning_rate: 0.0009\n",
      "    num_layers: 3\n",
      "    drop_rate: 0.35\n",
      "Epoch 0; Step 0; Loss 0.693147; Train acc: 0.515625; Dev acc 0.527344\n",
      "Epoch 5; Step 135; Loss 0.572307; Train acc: 0.777344; Dev acc 0.710938\n",
      "Epoch 10; Step 270; Loss 0.415383; Train acc: 0.941406; Dev acc 0.765625\n",
      "Epoch 15; Step 405; Loss 0.385440; Train acc: 0.968750; Dev acc 0.746094\n",
      "Epoch 20; Step 540; Loss 0.374295; Train acc: 0.980469; Dev acc 0.750000\n",
      "Epoch 25; Step 675; Loss 0.370466; Train acc: 0.976562; Dev acc 0.753906\n",
      "Epoch 30; Step 810; Loss 0.387523; Train acc: 0.984375; Dev acc 0.750000\n",
      "Best dev accuracy is 0.765625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hypermeters that are same for all tests\n",
    "input_size = vocab_size\n",
    "num_labels = 2\n",
    "hidden_dim = 24\n",
    "embedding_dim = 8\n",
    "batch_size = 256\n",
    "max_seq_length = 20\n",
    "num_epochs = 30\n",
    "\n",
    "# Try 60 combinations\n",
    "results = []\n",
    "for i in range(60):\n",
    "    print(\"(Search %d)\" % (i))\n",
    "    \n",
    "    learning_rate = random.choice([0.0008, 0.0009, 0.001, 0.0011, 0.0012])\n",
    "    num_layers = random.choice([1, 2, 3, 4])\n",
    "    drop_rate = random.choice([0.2, 0.25, 0.3, 0.35])\n",
    "    \n",
    "    print(\"    learning_rate: %.4f\" % learning_rate)\n",
    "    print(\"    num_layers: %d\" % num_layers)\n",
    "    print(\"    drop_rate: %.2f\" % drop_rate)\n",
    "\n",
    "    # Build and initialize the model\n",
    "    dan = DAN(vocab_size, embedding_dim, hidden_dim, num_labels, batch_size, num_layers, drop_rate)\n",
    "    dan.init_weights()\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss = nn.CrossEntropyLoss()  \n",
    "    optimizer = torch.optim.Adam(dan.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Build data iterators\n",
    "    training_iter = data_iter(training_set, batch_size)\n",
    "    train_eval_iter = eval_iter(training_set[0:500], batch_size)\n",
    "    dev_iter = eval_iter(dev_set[:500], batch_size)\n",
    "\n",
    "    # Train the model\n",
    "    acc = training_loop(batch_size, num_epochs, dan, loss, optimizer, training_iter, dev_iter, train_eval_iter)\n",
    "    results.append([acc, i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the top 6 performances (Accuracy > 0.83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.85546875, 33],\n",
       " [0.84765625, 7],\n",
       " [0.83984375, 2],\n",
       " [0.83984375, 23],\n",
       " [0.83984375, 36],\n",
       " [0.83203125, 56]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "top6 = sorted(results,key=itemgetter(0), reverse=True)[:6]\n",
    "top6 # List of lists where item0 is the accuracy and item1 is the Search ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Search 33)\n",
    "\n",
    "    learning_rate: 0.0008\n",
    "    num_layers: 2\n",
    "    drop_rate: 0.25\n",
    "    Best dev accuracy is 0.85546875\n",
    "\n",
    "  \n",
    "(Search 7)\n",
    "\n",
    "    learning_rate: 0.0008\n",
    "    num_layers: 2\n",
    "    drop_rate: 0.35\n",
    "    Best dev accuracy is 0.84765625\n",
    "\n",
    "(Search 2)\n",
    "\n",
    "    learning_rate: 0.0011\n",
    "    num_layers: 1\n",
    "    drop_rate: 0.20\n",
    "    Best dev accuracy is 0.83984375\n",
    "\n",
    "(Search 23)\n",
    "\n",
    "    learning_rate: 0.0010\n",
    "    num_layers: 1\n",
    "    drop_rate: 0.35\n",
    "    Best dev accuracy is 0.83984375\n",
    "\n",
    "(Search 36)\n",
    "\n",
    "    learning_rate: 0.0012\n",
    "    num_layers: 2\n",
    "    drop_rate: 0.30\n",
    "    Best dev accuracy is 0.83984375\n",
    "   \n",
    "(Search 56)\n",
    "\n",
    "    learning_rate: 0.0012\n",
    "    num_layers: 1\n",
    "    drop_rate: 0.30\n",
    "    Best dev accuracy is 0.83203125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Analysis__\n",
    "    - learning_rate: not much pattern found. Min and max values in the test range appears similarly.\n",
    "    - num_layers: 1 or 2 layers are common.\n",
    "    - drop_rate: 0.30 and 0.35 are most common but smaller values are present too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Part 3: short questions (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Name, and briefly describe, 3 other possible composition functions, instead of the DAN, you could use to build sentence representations.\n",
    "\n",
    "- LSTM: A type of RNN that is designed to avoid the long-term dependency problem. The modules repeat, and in each model there are four interacting layers (3 sigmoids, 1 tanh) (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- Infersent: Bi-LSTM + Max-Pooling encoder. The model processes sentences in both regular and reverse order. Then the output word representations are max-pooled over the time dimension to produce the final sentence embedding. (source: https://medium.com/data-from-the-trenches/how-deep-does-your-sentence-embedding-model-need-to-be-cdffa191cb53)\n",
    "- Smooth Inverse Frequency: Compute the weighted average of word vectors in the sentence and then remove the projections of the average vectors on their first singular vector to create sentence embeddings. (source: https://openreview.net/pdf?id=SyK00v5xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Explain how dropout regularizes a model.\n",
    "- Dropping out nodes or words has an effect of adding noise to the training process. When nodes are dropped during training, this creates $2^n$ different architectures for a model that has n nodes. This prevents the model from overfitting and the model becomes more robust.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What are the shortcomings for training for a fixed number of epochs? Give an alternative.\n",
    "- If the epoch size is too small, the model might not be trained sufficiently. If the epoch size is too big, the model may overfit resulting in a low accuracy for validation set. Therefore, epoch size should be decided carefully based on the model's performance instead of having a fixed number of epochs.\n",
    "- Early stopping is an alternative for fixed number of epochs. Given an arbitrary number of epochs, the model stops training once the model performance stops improving on the validation set. This solves the problem of underfitting or overfitting that may arise from traning for a fixed number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Explain why you might use random search rather than grid search.\n",
    "\n",
    "- Grid search will take too much time especially when there are many hyperparameters to tune\n",
    "- Random search has 95% chance of finding the top 5% optimal hyperparameters combinations when performed 60 iterations. This means that with much less iterations compared to grid search, random search can achieve almost as good performance. (source: http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Bonus (5 points): briefly describe the Nelder–Mead method and how you might use it to do hyperparamter tuning. What are the tradeoffs between using Nelder-Mead vs random search?_\n",
    "\n",
    "- The method: The Nelder–Mead method is an optimization method for finding a local minimum of a function of multiple variables. For functions where there are two variables, the simplex is a triangle. The method compares the values at each vertex of the triangle and replaces the worst performance vertex with a new vertex. So at every iteration, a new triangle is created where two of its vertices come from the last iteration and only one vertex is added. As the iteration proceeds, the triangle size becomes smaller and smaller and the iteration terminates once the optimal point is found.\n",
    "- Applying the method to hyperparameter tuning: Let each vertex of the triangle represent the different combinations of hyperparameters. Apply the Nelder–Mead method to find the optimal point where accuracy of the model is maximized.\n",
    "- Nelder-Mead vs Random Search: For Nelder-Mead, it is guaranteed to be able to find either local optimal or global optimal point. However, finding which vertices to choose from will be difficult and may require a big number of iterations. For random search, it has a high probability(95%) that the method will return a top 5% global optimal point even with few iterations(60 or more). However this also means that random search does not gurantee to find an optimal point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
